---
engine: knitr
---

# Hunt data {#sec-hunt-data}

**Required material**

- Read *Impact evaluation in practice*, Chapters 3 and 4, [@gertler2016impact]
- Read *Inventing the randomized double-blind trial: the Nuremberg salt test of 1835*, [@stolberg2006inventing]
- Read *Statistics and causal inference*, Parts 1-3, [@holland1986statistics]
- Read *Big tech is testing you*, [@fry2020big]
- Watch *Causal Inference Challenges in Industry: A perspective from experiences at LinkedIn*, [@yaxu]

<!-- - Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. and Freese, Jeremy, 2015, 'The generalizability of survey experiments', *Journal of Experimental Political Science*, 2 (2), pp. 109-138. -->

<!-- - Urban, Steve, Rangarajan Sreenivasan, and Vineet Kannan, 2016, 'it is All A/Bout Testing: The Netflix Experimentation Platform', *Netflix Technology Blog*, 29 April, https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15. -->

**Key concepts and skills**

- Concerns around measurement.
- Establishing treatment and control groups using randomization to estimate average treatment effects and understanding threats to these estimates.
- Understanding the requirements and implications of internal and external validity.
- Appreciating why informed consent and establishing the need for an experiment are critical.
- A/B testing and some of its nuances.
- Designing and implementing surveys.

**Key packages and functions**

- Base R
  - `head()`
  - `length()`
  - `mean()`
  - `names()`
  - `nrow()`
  - `rnorm()`
  - `sample()`
  - `set.seed()`
  - `sum()`
- Core `tidyverse` [@tidyverse]
  - `dplyr` [@citedplyr]
    - `count()`
    - `filter()`
    - `group_by()`
    - `if_else()`
    - `left_join()`
    - `mutate()`
    - `rename()`
    - `rowwise()`
    - `select()`
    - `summarize()`
    - `ungroup()`
  - `ggplot2` [@citeggplot]
    - `ggplot()`
    - `scale_fill_brewer()`
    - `theme_classic()`
    - `theme_minimal()`
  - `readr` [@citereadr]
    - `read_csv()`
  - `tibble` [@tibble]
    - `tibble()`
  - `tidyr` [@citetidyr]
    - `pivot_wider()`
  - `stringr` [@citestringr]
    - `str_detect()`


## Introduction

This chapter is about hunting data with experiments. This is a situation in which we can explicitly control and vary that which we are interested in. The advantage of this is that identifying and estimating an effect should be clear. There is a treatment group that is subject to that which we are interested in, and a control group that is not. These are randomly split before treatment. And so, if they end up different, then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely. Our ability to speak to whether we have measured the effect of the treatment affects our ability to speak to what effect that treatment could have.

By way of motivation, consider the situation of someone who moved to San Francisco in 2014--as soon as they moved the Giants won the World Series and the Golden State Warriors began a historic streak of World Championships. They then moved to Chicago, and immediately the Cubs won the World Series for the first time in a hundred years. They then moved to Massachusetts, and the Patriots won the Super Bowl again, and again, and again. And finally, they moved to Toronto, where the Raptors immediately won the World Championship. Should a city pay them to move, or could municipal funds be better spent elsewhere? 

One way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send them to live there for a year. With enough lifetimes, we could work it out. This would take a long time because we cannot both live in a city and not live in a city. This is the fundamental problem of causal inference: a person cannot be both treated and untreated. Experiments and randomized controlled trials are circumstances in which we try to randomly allocate some treatment, to have a belief that everything else was the same (or at least ignorable). We use the Neyman-Rubin potential outcomes framework to formalize the situation [@holland1986statistics]. 

A treatment, $t$, will often be a binary variable, that is either 0 or 1. It is 0 if the person, $i$, is not treated, which is to say they are in the control group, and 1 if they are treated. We will typically have some outcome, $Y_i$, of interest for that person, and that could be binary, multinomial, or continuous. For instance, it could be vote choice, in which case we could measure whether the person is: 'Conservative' or 'Not Conservative'; which party they support, say: 'Conservative', 'Liberal', 'Democratic', 'Green'; or maybe a probability of support.

A treatment is then causal if $(Y_i|t=0) \neq (Y_i|t=1)$. That is to say, the outcome for person $i$, given they were not treated, is different to their outcome given they were treated. If we could both treat and control the one individual at the one time, then we would know that it was only the treatment that had caused any change in outcome, as there could be no other factor to explain it. But the fundamental problem of causal inference remains: we cannot both treat and control the one individual at the one time. So when we want to know the effect of the treatment, we need to compare it with a counterfactual. The counterfactual, introduced in @sec-on-writing, is what would have happened if the treated individual were not treated. As it turns out, this means one way to think of causal inference is as a missing data problem, where we are missing the counterfactual.

As we cannot compared treatment and control in one individual, we instead compare the average of two groups---those treated and those not. We are looking to estimate the counterfactual at a group level because of the impossibility of doing it at an individual level. Making this trade-off allows us to move forward but comes at the cost of certainty. We must instead rely on randomization, probabilities, and expectations.

We usually consider a default of there being no effect and we look for evidence that would cause us to change our mind. As we are interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we will make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average, across the group, it is probably not the case. It is worth pointing out that we do not just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to: 

1. divide the dataset in two---treated and not treated---and have a binary effect column;
2. sum the column, then divide it by the length of the column; and
3. then look at the ratio.

This is an estimator, introduced in @sec-on-writing, which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. We can simulate data to illustrate the situation.

```{r}
#| eval: true
#| warning: false
#| message: false

library(tidyverse)

set.seed(853)

treat_control <- 
  tibble(
    binary_effect = sample(x = c(0, 1), size = 10, replace = TRUE)
  )

treat_control
```


```{r}
#| eval: true
#| warning: false
#| message: false

estimate <-
  sum(treat_control$binary_effect) / length(treat_control$binary_effect)
estimate
```

More broadly, to tell causal stories we need to bring together both theory and a detailed knowledge of what we are interested in [@Cunningham2021, p. 4]. In @sec-gather-data we discussed gathering data that we observed about the world. In this chapter we are going to be more active about turning the world into the data that we need. As the researcher, we will decide what to measure and how, and we will need to define what we are interested in. We will be active participants in the data generating process. That is, if we want to use this data, then as researchers we must go out and hunt it.

In this chapter we cover experiments, especially constructing treatment and control groups, and appropriately considering their results. We discuss some aspects of ethical behavior in experiments through reference to the abhorrent Tuskegee Syphilis Study and ECMO experiment. And we go through various case studies. We then turn to A/B testing, which is extensively used in industry, and consider a case study based on Upworthy data. Finally, we go through actually implementing a survey.

Ronald Fisher, the twentieth century statistician, and Francis Galton, the nineteenth century statistician, are the intellectual grandfathers of much of the work that we cover in this chapter. In some cases it is directly their work, in other cases it is work that built on their contributions. Both men believed in eugenics, amongst other things that are generally reprehensible. In the same way that art history acknowledges, say Caravaggio as a murderer, while also considering his work and influence, so to must statistics and the data sciences more generally concern themselves with this past, at the same time as we try to build a better future.


## Measurement

Measurement, and especially, the comparison of measurements, underpins all of data science due to its foundations in statistics. Deciding what to measure, and how to do it, is an especially challenging aspect of hunting data. @metrology [p. 44] define measurement as the 'process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity', where a quantity value is a 'number and reference together expressing magnitude of a quantity', such as the length of a given rod, or the mass of a given body [@metrology, p. 36]. It implies 'comparison of quantities, including counting of entities', and 'presupposes a description of the quantity commensurate with the intended use of a measurement result, a measurement procedure, and a calibrated measuring system...'. The definition of measurement implies that we have a variety of concerns including: instrumentation and units, and that we are interested in measurements that are appropriate and consistent.

Instrumentation refers to what we use to conduct the measurement. Thorough consideration of instrumentation is important because it determines what we can measure. For instance, @morange [p. 63] describes how the invention of microscopes in the sixteenth century led to the observation of cells by Robert Hooke, red blood cells by Jan Swammerdam, bacteria by Antonie van Leeuwenhoek, and capillaries by Marcello Malpighi. None of this was possible with, say, eye glasses. And consider the measurement of time---when we only had sundials, it would have been impossible to be much more specific about elapsed time than an hour or so. But the gradual development of more accurate instruments of timekeeping would eventually, through GPS, allow navigation accurate within meters.

A common instrument of measurement in RCTs is a survey, and we will see two examples of this later in this chapter. Another commonly-used instrument is sensors. For instance, we may be interested in temperature, humidity, or pressure. Sensors placed on satellites may be especially concerned with images, and such data are easily available from the Landsat Program. In the case of A/B testing, which we will discuss, extensive use is made of cookies, beacons, settings, and behavioral patterns. Another aspect of instrumentation is delivery. For instance, if using surveys, then should they be mailed or online? Should they be filled out by the respondent or by an enumerator?

The definition of measurement makes it clear that the second fundamental concern is a reference, which we will refer to as units. The choice of units is a crucial one, and related to both the research question of interest and instrumentation that are available. For instance, in the Tutorial in @sec-introduction we were concerned with measuring the growth of plants from seeds, and hair growth over the course of a week. Neither of these would be well served by considering an instrument focused on accuracies in the order of kilometers or miles. If we were using a ruler, then we may be able to measure, at best, millimeters, but with calipers, we might be able to consider tens of micrometers.

We aim to make our measurement appropriate and consistent.

Appropriateness means that the quantity that we are measuring is actually related to the estimand and research question of interest. Recall, from @sec-on-writing, that an estimand is the actual effect, such as the unknowable actual effect of smoking on life expectancy. This means that we need to ensure that we are measuring appropriate aspects, such as the number of cigarettes that were smoked and the number of years lived, rather than, say, opinion about smoking. We would find that measuring opinion about smoking would not help us answer the relationship between smoking and life expectancy and so it would not be an appropriate measure.

For some units, such as a meter, or a second, there is a clear definition. And while that definition evolves over time [@mitchell2022] it is widely agreed on. But for other aspects that we may wish to measure it is less clear and so appropriateness of the measurement becomes critical. For instance, how can we measure the quality of a university? One way, used by U.S. News, is to consider aspects such as class size, number of faculty with a PhD, number of faculty who are full-time, and student-faculty ratio. But an issue with such constructed measures in social settings is that it changes the incentives of those being measured. For instance, over the past 20 years, Columbia University has increased in these rankings from 18 in 1988 to 2 in 2022. But Michael Thaddeus, Professor of Mathematics, Columbia University, showed how there was a difference, in Columbia's favor, between what Columbia reported to U.S. News and what was available through other sources [@hartocollis2022]. 

Consistency draws on what is implied by the part of the definition of measurement that reads 'process of experimentally obtaining...'. It means that multiple measurements of one particular aspect, at one particular time, should be, *ceteris paribus*, essentially the same. If two enumerators independently count the number of shops on a street, then we would hope that their counts are the same. And if they were different then we would hope we could understand the reason for the difference. For instance, perhaps one enumerator, misunderstood the instructions and incorrectly considered only shops that were open.


:::{.callout-note}
## Oh, you think we have good data on that!

It is common for the pilot of a plane to announce the altitude to their passengers. But the notion and measurement of altitude is actually deceptively complicated, and underscores the fact that measurement occurs within a broader context [@skyfaring]. For instance, if we are interested in how many meters there are between the plane and the ground, then should we measure the difference between the ground and where the pilot is sitting, or to the bottom of the wheels, which would be useful for landing? What happens if we go over a mountain? Even if the plane has not descended, such a measure--the difference between the plane and the ground--would claim a reduction in altitude. So we may be interested in a comparison to sea level. But sea level changes because of the tide, and is different at different locations. As such, a common measure of altitude is flight level, which is determined by the amount of air pressure. And because air pressure is affected by weather, season, and location, the one flight level may be associated with very different numbers of meters to the ground over the course of a flight! Despite this, the measures of altitude used by planes serve their purpose of enabling relatively safe air travel.
:::

Measurement error is a difference between the value we observe and the actual value. Sometimes it is possible to verify certain responses. If the difference is consistent between those responses that we are able to verify and those that we are not, then we are able to estimate the extent of overall measurement error. For instance, @Sakshaug2010 considered a survey of alumni and compared responses about a respondent's grades, with university records. They found that the mode of the survey (telephone interview conducted by a human, telephone interview conducted by a computer, and an internet survey) affected the extent of the measurement error. 

This can be particularly pervasive when an enumerator fills out the survey form on behalf of the respondent. And this particularly manifests itself when there are racial differences. For instance, @davis1997nonrandom [p. 177] describes how Black people in America may limit the extent to which they describe their political and racial belief to White interviewers.

Another example is censored data, which is when we have some partial knowledge of the actual value. Right-censored data is when we know that the actual value is above than some observed value but we do not know by how much. For instance, immediately following the Chernobyl disaster, the only available instruments to measure radiation had a certain maximum limit. When the radiation was measured at that (maximum) level, it was considered manageable, even though the implication was that the actual value was much higher. 

Right-censored data are also often seen in medical studies of survival. For instance, if some experiment is conducted, and then patients are followed up for 10 years. At the end of that ten year period all we know is whether a patient lived at least 10 years, not the exact length of their life. Left-censored data is the opposite situation. For instance, consider a thermometer that only goes down to freezing. Even when the actual temperature was less than that, the thermometer would still register as freezing.

Truncated data is a slightly different situation in which we do not even record those values. For instance, 

Regardless of how good our data acquisition process is, there will be missing data. Non-response could be considered a variant of measurement error whereby we observe a null, even though there must be an actual value. But it is usually considered in its own right. And there are different extents of non-response: from refusing to even respond to the survey, through to just missing one question. Non-response is a key issue, especially with non-probability samples, because there is usually good reason to consider that people who do not respond are systematically different to those who do. And this serves to limit the extent to which the survey can be used to speak to more than just survey respondents. For instance, @gelman2016mythical go so far as to say that much of the changes in public opinion that are reported in the lead-up to an election are not people changing their mind, but differential non-response. The use of pre-notification and reminders may help address non-response in some circumstances [@koitsalu2018effects; @frandell2021effects].

In an ideal situation data are missing completely at random. This rarely occurs, but if it does, then the missing data can just be ignored with little concern because the sample should still be reflective of the broader population. It is more likely that data are missing at random and the extent to which we have to worry about that differs. For instance, if we are interested in the effect of gender on political support, then it may be that men are less likely to respond to surveys, but this is not related to who they will support. If that differential response is only due to being a man, and not related to political support, then we may be able to continue, provided we include gender in the regression. That said, the likelihood of this independence holding is low, and it is more likely, as in @gelman2016mythical, that there is a relationship between responding to the survey and political support. In that, more likely, case we have a more significant issue. One approach would be to consider additional explanatory variables. It is tempting to drop incomplete cases, but this may further bias the sample, and requires extensive justification. Data imputation should be considered only as a last resort, and again is likely to further bias the sample. Far better than either option is to rethink, and improve, the data collection process.







## Experiments and randomized controlled trials

### Randomization

Correlation can be enough in some settings [@hill1965environment], but in order to be able to make forecasts when things change, and the circumstances are slightly different we need to understand causation. Economics went through a credibility revolution in the 2000s [@angrist2010credibility] and there was similarly considerable increased use of experiments in political science in the 2000s and 2010s [@Druckman2021]. It is worth acknowledging that its embrace was not unanimous [@Deaton2010].

The key is the counterfactual: what would have happened in the absence of the treatment. Ideally, we could keep everything else constant, randomly divide the world into two groups, and treat one and not the other. Then we could be pretty confident that any difference between the two groups was due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (provided they are both big enough) should have the same characteristics as the population. Randomized controlled trials (RCTs) and A/B testing attempt to get us as close to this 'gold standard' as we can hope. 

When we, and others such as @athey2017state, use such positive language to refer to these approaches, we do not mean to imply that they are perfect. Just that they can be better than most of the other options. For instance, in @sec-causality-from-observational-data we will consider causality from observation data, and while this is sometimes all that we can do, the circumstances in which it is possible to evaluate both makes it clear that approaches based on observational data are usually second-best [@Gordon2019; @closeenoughaintgoodenough]. RCTs and A/B testing also bring other benefits, such as being able to design a study that focuses on a particular question and tries to uncover the mechanism by which the effect occurs [@alsan2021beyond]. But they are not perfect. In particular, one bedrock of experimental practice is that it be blinded, that is, a participant does not know whether they are in the treatment or control group. And ideally, double-blind, that is, even the research does not know. This is rarely the case for RCTs and A/B testing. Again, this is not to say they are not useful, after all in 1847 Semmelweis identified the benefit of having an intern wash their hands before delivering babies without a blinded study [@morange, p. 121].

What we hope to be able to do is to establish treatment and control groups that are the same, but for the treatment. This means that establishing the control group is critical because when we do that, we establish the counterfactual. We might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection into the treatment group. Either of these issues could result in biased estimates. We use randomization to go some way to addressing these.

To get started, we simulate a population, and then randomly sample from it. We will set it up so that half the population likes blue, and the other half likes white. And further, if someone likes blue then they almost surely prefer dogs, but if they like white then they almost surely prefer cats. The approach of heavily using simulation is a critical part of the workflow advocated in this book. This is because we know roughly what the outcomes should be from the analysis of simulated data. Whereas if we go straight to analyzing the real data, then we do not know if unexpected outcomes are due to our own analysis errors, or actual results. Another good reason it is useful to take this approach of simulation is that when you are working in teams the analysis can get started before the data collection and cleaning is completed. The simulation will also help the collection and cleaning team think about tests they should run on their data.

```{r}
set.seed(853)

number_of_people <- 5000

population <-
  tibble(
    person = c(1:number_of_people),
    favorite_color = sample(
      x = c("Blue", "White"),
      size  = number_of_people,
      replace = TRUE
    )
  ) |>
  mutate(
    prefers_dogs_to_cats =
      if_else(favorite_color == "Blue", "Yes", "No"),
    noise = sample(1:10, size = 1),
    prefers_dogs_to_cats =
      if_else(
        noise <= 8,
        prefers_dogs_to_cats,
        sample(c("Yes", "No"), 
               size = 1)
        )
  ) |> 
  select(-noise)


population

population |>
  group_by(favorite_color) |> 
  count()
```

Building on the terminology and concepts introduced in @sec-farm-data, we now construct a sampling frame that contains 80 per cent of the target population.

```{r}
set.seed(853)

frame <-
  population |>
  mutate(
    in_frame = sample(
      x = c(0, 1),
      size  = number_of_people,
      replace = TRUE,
      prob = c(0.2, 0.8)
  )) |>
  filter(in_frame == 1)

frame |>
  group_by(favorite_color) |> 
  count()
```

For now, we will set aside dog or cat preferences and focus on creating treatment and control groups on the basis of favorite color only.

```{r}
set.seed(853)

sample <-
  frame |>
  select(-prefers_dogs_to_cats) |>
  mutate(group = sample(
    x = c("Treatment", "Control"),
    size  = nrow(frame),
    replace = TRUE
  ))
```

When we look at the mean for the two groups, we can see that the proportions that prefer blue or white are very similar to what we specified (@tbl-dogsdtocats). 

```{r}
#| label: tbl-dogsdtocats
#| tbl-cap: "Proportion of the groups that prefer blue or white"

sample |>
  group_by(group, favorite_color) |>
  count() |>
  ungroup() |>
  group_by(group) |>
  mutate(prop = n / sum(n)) |>
  knitr::kable(
    col.names = c("Group", "Preferred color", "Number", "Proportion"),
    digits = 2,
    booktabs = TRUE,
    linesep = "",
    format.args = list(big.mark = ",")
  )
```

We randomized based on favorite color. But we should also find that we took dog or cat preferences along at the same time and will have a 'representative' share of people who prefer dogs to cats. Why should that happen when we have not randomized on these variables? Let us start by looking at our dataset (@tbl-dogstocats). 

```{r}
#| label: tbl-dogstocats
#| tbl-cap: "Proportion of the treatment and control group that prefer dogs or cats"

sample |> 
  left_join(frame |> select(person, prefers_dogs_to_cats), 
            by = "person") |>
  group_by(group, prefers_dogs_to_cats) |> 
  count() |>
  ungroup() |>
  group_by(group) |>
  mutate(prop = n / sum(n)) |>
  knitr::kable(
    col.names = c("Group", "Prefers dogs to cats", "Number", "Proportion"),
    digits = 2,
    booktabs = TRUE,
    linesep = "",
    format.args = list(big.mark = ",")
  )
```

It is exciting to have a representative share on 'unobservables'. (In this case, we do 'observe' them---to illustrate the point---but we did not select on them.) We get this because the variables were correlated. But it will break down in several ways that we will discuss. It also assumes large enough groups. For instance, if we considered specific dog breeds, instead of dogs as an entity, we may not find ourselves in this situation. To check that the two groups are the same, we look to see if we can identify a difference between the two groups based on observables, theory, experience, and expert opinion. In this case we looked at the mean, but we could look at other aspects as well. 

This would traditionally bring us to Analysis of Variation (ANOVA). ANOVA was introduced around one hundred years ago by Fisher while he was working on statistical problems in agriculture. (@Stolley1991 provides additional interesting background on Fisher.) This is less unexpected than it may seem because historically agricultural research was closely tied to statistical innovation. In particular, often statistical methods were designed to answer agricultural questions such as 'does fertilizer work?' and were only later adapted to clinical trials [@yoshioka1998use]. It was relatively easily to divide a field into 'treated' and 'non-treated', and the magnitude of any effect was large. While appropriate for that context, often these same statistical approaches are still taught today in introductory material, even when they are being applied to very different circumstances to those they were designed for. For instance, these days, many researchers worry more about data quantity than data quality, and effect sizes may be small, which creates issues for these approaches [@Bradley2021]. It almost always pays to take a step back and think about what is being done and whether it is appropriate to these circumstances. We mention ANOVA here because of its importance historically, but it is a variant of linear regression and, in general, we would usually not use ANOVA day-to-day these days. There is nothing wrong with it in the right circumstances. But it is more than one-hundred years old and the number of modern use-cases where it is the best option is small. A better option, in many cases, would be to build a hierarchical model, which we cover in @sec-its-just-a-linear-model.

Nonetheless, if we were to use ANOVA here, we would approach it with the expectation that the groups are from the same distribution and could conduct it using `aov()`. In this case, at the five percent level, we would fail to reject a null hypothesis that the samples are the same.


### Treatment and control

If the treated and control groups are the same in all ways and remain that way, but for the treatment, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between the groups in that study. Internal validity means that our estimates of the effect of the treatment speak to the treatment and not some other aspect. It means that we can use our results to make claims about what happened in the experiment.

If the group to which we applied our randomization were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further could have external validity. That would mean that the difference that we find does not just apply in our own experiment, but also in the broader population. External validity means that we can use our experiment to make claims about what would happen outside the experiment. It is randomization that has allowed that to happen. It is worth noting that in practice we would not just rely on one experiment, but would instead consider that a contribution to a broader evidence-collection effort [@Duflo2020 p. 1955].


:::{.callout-note}
## Shoulders of giants

Dr Esther Duflo is Abdul Latif Jameel Professor of Poverty Alleviation and Development Economics at MIT.
After taking a PhD in Economics from MIT in 1999, she remained at MIT as an assistant professor, being promoted to full professor in 2003. 
One area of her research is applications in economic development and she uses randomized controlled trials to understand how to address poverty.
One of her important books is @pooreconomics and one of her important papers is @banerjee2015miracle.
She was awarded the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 2019 and @duflonobelwriteup provides an excellent overview of her work.
:::

But this means we need randomization twice. Firstly, into the group that was subject to the experiment, and then secondly, between treatment and control. How do we think about this randomization, and to what extent does it matter?

We are interested in the effect of being treated. It may be that we charge different prices, which would be a continuous treatment variable, or that we compare different colors on a website, which would be a discrete treatment variable. Either way, we need to make sure that all the groups are otherwise the same. How can we be convinced of this? One way is to ignore the treatment variable and to examine all other variables, looking for whether we can detect a difference between the groups based on any other variables. For instance, if we are conducting an experiment on a website, then are the groups roughly similar in terms of, say:

- Microsoft and Apple users?
- Safari, Chrome, and Firefox users?
- Mobile and desktop users?
- Users from certain locations?

Further, are the groups representative of the broader population? These are all threats to the validity of our claims.

But if done properly, that is if the treatment is truly independent, then we can estimate the average treatment effect (ATE). In a binary treatment variable setting this is:
$$\mbox{ATE} = \mathbb{E}[Y|t=1] - \mathbb{E}[Y|t=0].$$

That is, the difference between the treated group, $t = 1$, and the control group, $t = 0$, when measured by the expected value of the outcome, $Y$. The ATE becomes the difference between the two conditional expectations.

To illustrate this concept, we first simulate some data that shows a difference of one between the treatment and control groups.

```{r}
#| label: exampleate

set.seed(853)

ate_example <- tibble(person = c(1:1000),
                      was_treated = sample(
                        x = c("Yes", "No"),
                        size  = 1000,
                        replace = TRUE
                      ))

# Make outcome a bit more likely if treated.
ate_example <-
  ate_example |>
  rowwise() |>
  mutate(outcome = if_else(
    was_treated == "No",
    rnorm(n = 1, mean = 5, sd = 1),
    rnorm(n = 1, mean = 6, sd = 1)
  ))
```

We can see the difference, which we simulated to be one, between the two groups in @fig-exampleatefig. And we can compute the average between the groups and then the difference to see also that we roughly get back the result that we put in (@tbl-exampleatetable).

```{r}
#| fig-cap: "Simulated data showing a difference between the treatment and control group"
#| label: fig-exampleatefig

ate_example |>
  ggplot(aes(x = outcome,
             fill = was_treated)) +
  geom_histogram(position = "dodge",
                 binwidth = 0.2) +
  theme_minimal() +
  labs(x = "Outcome",
       y = "Number of people",
       fill = "Person was treated") +
  scale_fill_brewer(palette = "Set1")
```


```{r}
#| label: tbl-exampleatetable
#| tbl-cap: "Average difference between the treatment and control groups for data simulated to have an average difference of one"

ate_example |>
  group_by(was_treated) |>
  summarize(mean = mean(outcome)) |>
  pivot_wider(names_from = was_treated, values_from = mean) |>
  mutate(difference = Yes - No) |>
  knitr::kable(
    col.names = c("Average for treated", "Average for not treated", "Difference"),
    digits = 2,
    booktabs = TRUE,
    linesep = ""
  )
```

Unfortunately, there is often a difference between simulated data and reality. For instance, an experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment; but it cannot be too short otherwise we cannot measure longer term outcomes. We cannot have a 'representative' sample across every facet of a population, but if not, then the treatment and control may be different. Practical difficulties may make it difficult to follow up with certain groups and so we end up with a biased collection. Some questions to explore when working with real experimental data include: 

- How are the participants being selected into the frame for consideration? 
- How are they being selected for treatment? We would hope this is being done randomly, but this term is applied to a variety of situations. Additionally, early 'success' can lead to pressure to treat everyone, especially in medical settings.
- How is treatment being assessed?
- To what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish definitions, and the power imbalance between those making these decisions and those being treated should be considered.

Bias and other issues are not the end of the world. But we need to think about them carefully. Selection bias, introduced in @sec-on-writing, can be adjusted for, but only it if it recognized. For instance, how would the results of a survey about the difficulty of a university course differ if only students who completed the course were surveyed, and not those who dropped out? We should always work to try to make our dataset as representative as possible when we are creating it, but it may be possible to use a model to adjust for some of the bias *ex post*. For instance, if there was a variable that was correlated with say, attrition, then it could be added to the model either by-itself, or as an interaction. Similarly, if there was correlation between the individuals. For instance, if there was some 'hidden variable' that we did not know about that meant some individuals were correlated, then we could use wider standard errors. This needs to be done carefully and we discuss this further in @sec-causality-from-observational-data. That said, if such issues can be anticipated, then it can be better to change the experiment. For instance, perhaps it would be possible to stratify by that variable.


### Fisher's tea party

Fisher introduced an experiment designed to see if a person can distinguish between a cup of tea where the milk was added first, or last. We begin by preparing eight cups of tea: four with milk added first and the other four with milk added last. We then randomize the order of all eight cups. We tell the taster, whom we will call 'Ian', about the experimental set-up: there are eight cups of tea, four of each type, he will be given cups of tea in a random order, and his task is to group them into two groups.

One of the nice aspects of this experiment is that we can do it ourselves. There are a few things to be careful of in practice, including that: the quantities of milk and tea are consistent; the groups are marked in some way that the taster cannot see; and the order is randomized. 

Another nice aspect of this experiment is that we can calculate the chance that Ian is able to randomly get the groupings correct. To decide if his groupings were likely to have occurred at random, we need to calculate the probability this could happen. First, we count the number of successes out of the four that were chosen. @fisherdesignofexperiments [p. 14] says there are: ${8 \choose 4} = \frac{8!}{4!(8-4)!}=70$ possible outcomes.

We are asking Ian to group the cups, not to identify which is which, and so there are two ways for him to be perfectly correct. He could either correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70). This means the probability of this event is: $\frac{2}{70} \approx 0.028$ or about 3 per cent. 

As @fisherdesignofexperiments [p.15] makes clear, this now becomes a judgement call. We need to consider the weight of evidence that we require before we accept the groupings did not occur by chance and that Ian was well-aware of what he was doing. We need to decide what evidence it takes for us to be convinced. If there is no possible evidence that would dissuade us from the view that we held coming into the experiment, say, that there is no difference between milk-first and tea-first, then what is the point of doing an experiment? We expect that if Ian got it completely right, then the reasonable person would accept that he was able to tell the difference. 

What if he is almost-perfect? By chance, there are 16 ways for a person to be 'off-by-one'. Either Ian thinks there was one cup that was milk-first when it was tea-first---there are, ${4 \choose 1} = 4$, four ways this could happen---or he thinks there was one cup that was tea-first when it was milk-first---again, there are, ${4 \choose 1}$ = 4, four ways this could happen. These outcomes are independent, so the probability is $\frac{4\times 4}{70} \approx 0.228$. And so on. Given there is an almost 23 per cent chance of being off-by-one just by randomly grouping the teacups, this outcome probably would not convince us that Ian can tell the difference between tea-first and milk-first.

What we are looking for, in order to claim something is experimentally demonstrable is the results of not just it being shown once, but instead to come to know the features of an experiment where such a result is reliably found [@fisherdesignofexperiments p. 16]. We are looking to thoroughly interrogate our data and our experiments, and to think precisely about the analysis methods we are using. Rather than searching for meaning in constellations of stars, we want to make it as easy as possible for others to reproduce our work. It is only in that way that our conclusions stand a chance of holding up in the long-term.




### Informed consent and establishing the need for an experiment

One of the foundations of ethical experimental practice is informed consent and ensuring that an experiment is actually needed. We now detail two cases where human life was potentially lost to remind us of the importance of these foundations. One issue with experiments in medical settings is that the weight of evidence is measured in lost lives. Ethical practice in experiments develops because of the many people who may have unnecessarily lost their life. Two cases that have dramatically informed practice are the Tuskegee Syphilis Study and the ECMO experiments.

The Tuskegee Syphilis Study is an infamous medical trial that began in 1932. As part of this experiment 400 Black Americans with syphilis, and a control group without, were not given appropriate treatment, nor even told they had syphilis (in the case of the treatment group), well after a standard treatment for syphilis was established and widely available [@brandt1978racism; @tuskegeeandthehealthofblackmen]. Like the treatment group, the control group were also given non-effective drugs. These financially-poor Black Americans in the US South were offered minimal compensation and not told they were part of an experiment [@brandt1978racism; @tuskegeeandthehealthofblackmen]. Further, extensive work was undertaken to ensure the men would not receive treatment from anywhere, including writing to local doctors and the local health department. Incredibly, after some of the men were drafted and told to immediately get treatment, the draft board complied with a request to have the men excluded from treatment [@brandt1978racism, p. 25]. By the time the study was stopped in 1972, more than half of the men were deceased and many of deaths were from syphilis-related causes [@tuskegeeandthehealthofblackmen].

The effect of the Tuskegee Syphilis Study was felt not just by the men in the study, but more broadly. @tuskegeeandthehealthofblackmen found that it is associated with a decrease in life expectancy at age 45 of up to 1.5 years for Black men. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. @brandt1978racism [p. 27] says:

> In retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process... [T]he notion that science is a value-free discipline must be rejected. The need for greater vigilance in assessing the specific ways in which social values and attitudes affect professional behavior is clearly indicated.

:::{.callout-note}
## Shoulders of giants

Dr Marcella Alsan is a Professor of Public Policy at Harvard University. She has an MD from Loyola University and took a PhD in Economics from Harvard University in 2012 after which she was appointed as an assistant professor at Stanford, being promoted to full professor in 2019 when she moved to Harvard. One area of her research is health inequality, and one particularly important paper is @tuskegeeandthehealthofblackmen, which we discussed above. She was awarded a MacArthur Foundation Fellowship in 2021.
:::


Turning to the evaluation of extracorporeal membrane oxygenation (ECMO), @ware1989investigating describes how they viewed ECMO as a possible treatment for persistent pulmonary hypertension in newborns. They enrolled 19 patients and used conventional medical therapy on ten of them, and ECMO on nine of them. It was found that six of the ten in the control group survived while all in the treatment group survived. @ware1989investigating used randomized consent whereby only the parents of infants randomly selected to be treated with ECMO were asked to consent. 

@ware1989investigating is concerned with 'equipoise', by which they refer to a situation in which there is genuine uncertainty about whether the treatment is more effective than existing procedures. They further note that in medical settings even if there is initial equipoise it could be undermined if the treatment is found to be effective early in the study. @ware1989investigating describe how after the results of these first 19 patients, randomization stopped and only ECMO was used. The recruiters and those treating the patients were initially not told that randomization had stopped. It was decided that this complete allocation to ECMO would continue 'until either the 28th survivor or the 4th death was observed'. After 19 of 20 additional patients survived the trial was terminated. So the actual result of the experiment was divided into two phases: in the first there was randomized use of ECMO, and in the second only ECMO was used.

One approach in these settings is a 'randomized play-the-winner' rule following @wei1978randomized. Treatment is still randomized, but the weight of probability shifts, with each successful treatment to make treatment more likely and there is some stopping rule. But @berry1989investigating argues that far from the need for a more sophisticated stopping rule, there was actually no need for the study at all, because equipoise never existed. @berry1989investigating re-visits the literature mentioned by @ware1989investigating and finds extensive evidence that ECMO was known to be effective. @berry1989investigating points out that there is almost never complete consensus and so one could almost always argue, inappropriately, for the existence of equipoise even in the face of a substantial weight of evidence. @berry1989investigating further criticizes @ware1989investigating for the use of randomized consent because of the potential that there may have been different outcomes for the infants subject to conventional therapy had their parents known there were other options. And instead, @berry1989investigating argues for the need for comprehensive patient registries, enabling the analysis of large datasets.

While the Tuskegee Syphilis Study and ECMO experiments may seem quite far from our present circumstances, Dr Monica Alexander, Assistant Professor, University of Toronto explains that while it may be illegal to do this exact research these days, it does not mean that unethical research does not still happen. She says that we see it in machine learning applications in health and other areas and while we are not meant to explicitly discriminate and we are meant to get consent, it does not mean that we cannot implicitly discriminate without any type of buy-in at all. For instance, @obermeyer2019dissecting describes how US health care systems use algorithms to score the severity of how sick a patient is. They show that for the same score, Black patients are actually sicker, and that if Black patients were scored in the same way as White patients, then they would receive considerably more care. They find that the discrimination occurs because the algorithm is based on health care costs, rather than sickness. But because access to healthcare is unequally distributed between Black and White patients, the algorithm, however inadvertently, perpetuates racial bias.


## RCT examples


### The Oregon Health Insurance Experiment

In the US, unlike many developed countries, basic health insurance is not necessarily available to all residents even those on low incomes. The Oregon Health Insurance Experiment involved low-income adults in Oregon, a state in the north-west of the US, from 2008 to 2010 [@finkelstein2012oregon]. 

Oregon funded 10,000 places in the state-run Medicaid program, which provides health insurance for people with low incomes. A lottery was used to allocate these places and this was judged fair because it was expected, correctly as it turned out, that demand for places would exceed the supply. In the end, 89,824 individuals signed up.

The draws were conducted over a six-month period and 35,169 individuals were selected (the household of those who actually won the draw was given the opportunity) but only 30 per cent of them turned out to be eligible and complete the paperwork. The insurance lasted indefinitely. This random allocation of insurance allowed the researchers to understand the effect of health insurance. 

The reason that this random allocation is important is that it is not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who do not. That decision is 'confounded' with other variables and results in selection bias. 

As the opportunity to apply for health insurance was randomly allocated, the researchers were able to evaluate the health and earnings of those who received health insurance and compare them to those who did not. To do this they used administrative data, such as hospital discharge data, matched credit reports, and, uncommonly, mortality records. This collection of data is limited, and so they included a survey conducted by mail.

The specifics of this are not important, and we will have more to say in @sec-its-just-a-linear-model, but they use a statistical model, @eq-oregon, to analyze the results [@finkelstein2012oregon]:

$$
y_{ihj} = \beta_0 + \beta_1\mbox{Lottery} + X_{ih}\beta+2 + V_{ih}\beta_3 + \epsilon_{ihj} 
$$ {#eq-oregon}

@eq-oregon explains various $j$ outcomes (such as health) for an individual $i$ in household $h$ as a function of an indicator variable as to whether household $h$ was selected by the lottery. Hence, it is the $\beta_1$ coefficient that is of particular interest. That is our estimate of the mean difference between the treatment and control groups. To complete the specification of @eq-oregon, $X_{ih}$ is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, $V_{ih}$ is a set of variables that are not correlated with the lottery. These variables include demographics, hospital discharge and lottery draw.

Similar to earlier studies such as @randhealth, @finkelstein2012oregon found that the treatment group was 25 per cent more likely to have insurance than the control group. The treatment group used more health care including both primary and preventive care as well as hospitalizations but had lower out-of-pocket medical expenditures. More generally, the treatment group reported better physical and mental health.


<!-- ### Student Coaching: How Far Can Technology Go? -->

<!-- There is a general concern about students dropping out of university before they finish their degree. If you work one-on-one with a student then this addresses the issue. But that does not scale. The point of this experiment was to see if technology-based options could be more efficient. The focus was the University of Toronto, and in particular first-year economics courses in Fall 2015. -->

<!-- The intervention was administered to students as part of an economics class. Students received 2 per cent of their grade for completing the exercise. The specific exercise depended on the group of the student. The intervention involved three treatments as well as a control group that was just given a Big Five personality traits test. Additional information that was obtained included 'the highest level of education obtained by students parents, the amount of education they expect to obtain, whether they are first-year or international students, and their work and study time plans for the upcoming year.' [@Oreopoulos2018, p. 6]. -->

<!-- The treatments were [@Oreopoulos2018, p. 4]: -->

<!-- 1. '[A] one-time, online exercise completed during the first two weeks of class in the fall'. This exercise was 'designed to get them thinking about the future they envision and the steps they could take in the upcoming year at U of T to help make that future a reality. They were told that the exercise was designed for their benefit and to take their time while completing it. The online module lasted approximately 60 to 90 minutes and led students through a series of writing exercises in which they wrote about their ideal futures, both at work and at home, what they would like to accomplish in the current year at U of T, how they intend on following certain study strategies to meet their goals, and whether they want to get involved with extracurricular activities at the university' [@Oreopoulos2018, p. 6]. -->
<!-- 2. '[T]he online intervention plus text and email messaging throughout the full academic year'. This involved the students being given 'the opportunity to provide their phone numbers and participate in a text and email messaging campaign lasting throughout both the fall semester in 2015 and the winter semester in 2016' [@Oreopoulos2018, p. 8]. All students in this group got the emails, but only those that provided phone numbers got the messages. They were able to opt out, but 'few chose to do so' [@Oreopoulos2018, p. 8]. This was a two-way interaction in which students could ask questions. Some asked for the 'locations of certain facilities on campus or how to stay on residence during the holiday break, while others said they need help with English skills or specific courses. Some students expressed relatively deep emotions, such as feeling anxious about family pressure to succeed in school or from doing poorly on an evaluation' [@Oreopoulos2018, p. 9]. A response was usually given within an hour. -->
<!-- 3. '[T]he online intervention plus one-on-one coaching in which students are assigned to upper-year undergraduate coaches'. 'Coaches were available to meet with students to answer any questions via Skype, phone, or in person, and would send their students regular text and email messages of advice, encouragement, and motivation, much like the You@UofT program described above. In contrast to the messaging program, however, coaches were instructed to be proactive and regularly monitor their students progress. Whereas the You@UofT program attempts to nudge students in the right direction with academic advice, coaches play a greater support role, sensitively guiding students through problems.' [@Oreopoulos2018, p. 11]. This coaching program was only available at UTM. 'Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.' -->

<!-- > Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.  -->
<!-- > -->
<!-- > [@Oreopoulos2018, p. 14] -->

<!-- The model they consider is [@Oreopoulos2018, p. 15]: -->

<!-- \begin{equation} -->
<!-- y_{ij} = \alpha + \beta_1\mbox{Online}_i + \beta_2\mbox{Text}_i + + \beta_3\mbox{Coach}_i + \delta_j + \mu \mbox{First year}_i + \epsilon_{ij} (\#eq:toronto) -->
<!-- \end{equation} -->

<!-- Equation \@ref(eq:toronto) explains the outcome of student $i$ at campus $j$ based on 'indicators for each of the three treatment exercises students were given, campus fixed effects, and a first-year student indicator.' The main parameters of interest are $\beta_1$, $\beta_2$ and $\beta_3$. The main outcomes were course grades, GPA, credits earned and failed. -->

<!-- It was found, that the one-on-one coaching 'increased grades by approximately 5 percentage points', while the other treatments had 'had no detectable impact'. One set of results are summarised in @fig-torontointervention). -->

<!-- 
#| fig-cap: "Example of the results of the intervention."} --
#| echo: false
#| label: torontointervention
<!-- ![](figures/toronto_intervention.png")) -{#fig- width=95% fig-align="center"}
# out-width: "90"
>
<!-- ``` -->

<!-- The results are important not only in a teaching context, but also for businesses hoping to retain customers. More papers are available [here](https://www.povertyactionlab.org/evaluation/student-coaching-how-far-can-technology-go). -->




### Civic Honesty Around The Globe

Trust is not something that we think regularly about, but it is actually fairly fundamental to most interactions, both economic and personal. For instance, many of us get paid after we do some work--we are trusting our employer will make good; and vice versa--if you get paid in advance then they are trusting you. In a strictly naive, one-shot, transaction-cost-less world, this does not make sense. If you get paid in advance, the incentive is for you to take the money and run in the last pay period before you quit, and through backward induction everything falls apart. Of course, we do not live in such a world. For one thing there are transaction costs, for another, generally, we have repeated interactions, and finally, the world usually ends up being fairly small.

Understanding the extent of honestly in different countries may help us to explain economic development and other aspects of interest such as tax compliance, but it is hard to measure. We cannot ask people how honest they are--the liars would lie, resulting in a lemons problem [@akerlof1978market]. To get around this @cohn2019civic conduct an experiment in 355 cities across 40 countries where they 'turn in' a wallet that is either empty or contains the local equivalent of US$13.45. They are interested in whether the 'recipient' attempts to return the wallet. They find that generally wallets with money were more likely to be returned [@cohn2019civic, p. 1].

In total @cohn2019civic 'turn in' 17,303 wallets to various institutions including banks, museums, hotels, and police stations. The importance of such institutions to an economy is generally well-accepted [@acemoglu2001colonial] and they are common across most countries. Importantly, for the experiment, they usually have a reception area where the wallet could be turned in [@cohn2019civic, p. 1].

In the experiment a research assistant turned in the wallet to an employee at the reception area, using a set form of words. The research assistant had to note various features of the setting, such as the gender, age-group, and busyness of the 'recipient'. The wallets were transparent and contained a key, a grocery list, and a business card with a name and email address. The outcome of interest was whether an email was sent to the unique email address on the business card in the wallet. The grocery list was included to signal that the owner of the wallet was a local. The key was included as something that was only useful to the owner of the wallet, and never the recipient, in contrast to the cash, to adjust for altruism. The language and currency were adapted to local conditions. 

The primary treatment in the experiment is whether the wallet contained money or not. The key outcome was whether the wallet was attempted to be returned or not. It was found that the median response time was 26 minutes, and that if an email was sent then it usually happened within a day [@cohn2019civicaddendum, p. 10]. 

Using the data for the paper that is made available [@walletsdata] we can see that considerable differences were found between countries (@fig-wallets). But in almost all countries wallets with money were more likely to be returned than wallets without. The experiments were conducted across 40 countries, which were chosen based on them having enough cities with populations of at least 100,000, as well as the ability for the research assistants to safely visit and withdraw cash. Within those countries, the cities were chosen starting with the largest ones and there were usually 400 observations in each country [@cohn2019civicaddendum, p. 5]. @cohn2019civic further conducted the experiment with the equivalent of US$94.15 in three countries--Poland, the UK, and the US--and found that reporting rates further increased. 

```{r}
#| fig-cap: "Comparison of the proportion of wallets handed in, by country, depending on whether they contained money"
#| echo: false
#| label: fig-wallets
#| message: false
#| warning: false

library(tidyverse)

wallet_data <- read_csv(here::here("inputs/data/behavioral_data.csv"))

wallet_data |> 
  filter(cond %in% c(0, 1)) |> 
  count(Country, cond, response) |> 
  group_by(Country, cond) %>%
  mutate(freq = n / sum(n) * 100) |> 
  filter(response == 100) |> 
  mutate(cond = if_else(cond == 0, "No", "Yes")) |> 
  mutate(cond = factor(cond)) |> 
  ggplot() +
  geom_point(aes(x=reorder(Country, -freq, FUN=mean), y = freq, color = cond)) +
  labs(x = "Country",
       y = "Reporting rate (%)",
       color = "Contained money?") +
  scale_color_brewer(palette = "Set1") +
  coord_flip() +
  theme_classic()
```

In addition to the experiments, @cohn2019civic conducted surveys that allowed them to understand some reasons for their findings. During the survey, participants were given one of the scenarios and then asked to answer questions. The use of surveys also allowed them to be specific about the respondents. The survey involved 2,525 respondents (829 in the UK, 809 in Poland, and 887 in the US) [@cohn2019civicaddendum, p. 36].  Participants were chosen using attention checks and demographic quotas based on age, gender, and residence, and they received US $4.00 for their participation [@cohn2019civicaddendum, p. 36]. 


## A/B testing

The past two decades has probably seen the most experiments ever run, likely by several orders of magnitude. This is because of the extensive use of A/B testing at tech firms [@Kohavi2012]. For a long time decisions such as what font to use were based on the Highest Paid Person's Opinion (HIPPO) [@abtestswired]. These days, many large tech companies have extensive infrastructure for experiments, and they term them A/B tests because of the comparison of two groups: one that gets treatment A and the other that either gets treatment B or does not see any change [@Salganik2018 p. 185]. Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. This is especially the case if you use apps like TikTok. While, at their heart, they are just experiments that use sensors to measure data that need to be analyzed, they have many special features that are interesting in their own light.

@kohavi [p. 3] discuss the example of Microsoft's search engine Bing. They used A/B testing to examine how much content of an ad to display. Based on these tests, they ended up increasing the amount of content displayed for each ad. This caused revenue to increase by 12 per cent, or around $100 million annually in the US, without any significant trade-off being measured.

We use the term A/B test to strictly refer to the situation in which we are primarily implementing an experiment through a technology stack about something that is primarily of the internet, for instance a change to a website or similar and measured with sensors rather than a survey. While at their heart they are just experiments, A/B testing has a range of specific concerns. There is something different about doing tens of thousands of small experiments all the time, compared with the RCT set-up of conducting one experiment over the course of months. Additionally, tech firms have such distinct cultures that it can be difficult to implement A/B testing. This makes culture, relationship building, and nuance important when shifting toward implementing experiments. Sometimes it can be easier to experiment by not delivering, or delaying, a change that has been decided to create a control group rather than a treatment group [@Salganik2018 p. 188]. This may especially be the case where there is a particularly incorrigible HIPPO. Often the most difficult aspect of A/B testing and conducting experiments more generally is not the statistics, it is the politics.

The first aspect of concern is the delivery of the A/B test [@kohavi, p. 153-161]. In the case of an experiment, it is usually clear how it is being delivered. For instance, we may have the person come to a doctor's clinic and then inject them with either a drug or a placebo. But in the case of A/B testing, it is less obvious. For instance, should it be run 'server-side', meaning to make a change to a website, or 'client-side', meaning to change an app. This decision affects our ability to both conduct the experiment and to gather data from it. 

In the case of the effect of conducting the experiment, it is relatively easy and normal to update a website all the time. This means that small changes can be easily implemented if the experiment is conducted server-side. But in the case of a client-side implementation of an app, then conducting an experiment becomes a bigger deal. For instance, the release may need to go through an app store, and this usually does not happen all the time. Instead, it would need to be part of a regular release cycle. There is also a selection concern because some users will not update their app and there is the possibility that they are different to those that do regularly update the app. 

Turning to the effect of the delivery decision on our ability to gather data from the experiment. Again, server-side is less of a big deal because we get the data anyway as part of the user interacting with the website. But in the case of an app, the user may use the app offline or with limited data upload, which then requires a data transmission protocol or caching, but this then could affect user experience, especially as some phones place limits are various aspects.

The effect of all this is that we need to plan. For instance, results are unlikely to be available the day after a change to an app, whereas they are likely available the day after a change to a website. Further, we may need to consider our results in the context of different devices and platforms, potentially using, say, multilevel regression which will be covered in @sec-its-just-a-linear-model.

The second aspect of concern is 'instrumentation' or the method of measurement [@kohavi, p. 162 - 165]. When we conduct a traditional experiment then we might, for instance, ask respondents to fill out a survey. But this is usually not done with A/B testing. One approach is to put a cookie on the user's device, but different users will clear these at different rates. Another approach is to use a beacon, such as forcing the user to download a tiny image from our server, so that we know when they have completed some action. For instance, this is a commonly used approach to know when a user has opened an email. There are practical concerns around when the beacon loads, for instance, if it is before the main content loads then the user experience may be worse, but if it is after then our sample may be biased.

The third aspect of concern is what are we randomizing over [@kohavi, p. 162 - 165]. In the case of traditional experiments, this is usually clear and it is often a person, but sometimes various groups of people. But in the case of A/B testing it can be less clear. For instance, are we randomizing over the page, the session, or the user? 

To think about this, let us consider color. For instance, say we are interested in whether we should change our logo from red to blue on the homepage. If we are randomizing at the page level, then if the user goes to some other page of our website, and then back to the homepage the logo could be back to red. If we are randomizing at the session level, then while it could be blue while they use the website this time, if they close it and come back then it could be red. Finally, if we are randomizing at a user level then possibly it would always be red for one used, but always blue for another.

The extent to which this matters depends on a trade-off between consistency and importance. For instance, if we are A/B testing product prices then consistency is likely a feature. But if we are A/B testing background colors then consistency might not be as important. On the other hand, if we are A/B testing the position of a log-in button then it might be important that we not move that around too much for the one user, but between users it might matter less.

Interestingly, in A/B testing, as in traditional experiments, we are concerned that our treatment and control groups are the same, but for the treatment. In the case of traditional experiments, we satisfy ourselves of this by making conducting analysis on the basis of the data that we have after the experiment is conducted. That is usually all we can do because it would be weird to treat or control both groups. But in the case of A/B testing, the pace of experimentation allows us to randomly create the treatment and control groups, and then check, before we subject the treatment group to the treatment, that the groups are the same. For instance, if we were to show each group the same website, then we would expect the same outcomes across the two groups. If we found different outcomes then we would know that we may have a randomization issue [@taddy2019, p. 129].

One of the interesting aspects of A/B testing is that we are usually running them not because we desperately care about the specific outcome, but because that feeds into some other measure that we care about. For instance, do we care whether the website is quite-dark-blue or slightly-dark-blue? Probably not, but we probably care a lot about the company share price. But then what if picking the best blue comes at a cost to the share price? That example is a bit contrived, so let us pretend that we work at a food delivery app and we are concerned with driver retention. Say we do some A/B tests and find that drivers are always more likely to be retained when they can deliver food to the customer faster. Our finding is that faster is better, for driver retention, always. But one way to achieve faster deliveries, is for them to not put the food into a hot box that would maintain the food's temperature. Something like that might save 30 seconds, which is significant on a 10-15 minute delivery. Unfortunately, although we would decide to encourage that that on the basis of A/B tests designed to optimize driver-retention, such a decision would likely make the customer experience worse. If customers receive cold food, when it is meant to be hot, then they may stop using the app, which would ultimately be very bad for the business.

This trade-off may be obvious when we run the driver experiment if we were to look at customer complaints. It is possible that on a small team we would be exposed to those tickets, but on a larger team we may not be. Ensuring that A/B tests are not resulting in false optimization is especially important. And not something that we typically have to worry about in normal experiments.


:::{.callout-note}
## Shoulders of giants

Dr Susan Athey is the Economics of Technology Professor at Stanford University. After taking a PhD in Economics from Stanford in 1995, she joined MIT as an assistant professor, being promoted to full professor in 2006 when she moved to Harvard. One area of her research is applied economics, and one particularly important paper is @Abadie2017, which considers when do standard errors need to be clustered, and another is @Athey2017, which considers how to analyze randomized experiments. In addition to her academic appointments, she has worked at Microsoft and other tech firms and been extensively involved in running experiments at tech firms. She was awarded the John Bates Clark Medal in 2007.
:::


The trouble with much of A/B testing is that it is done by firms and so we typically do not have datasets that we can use. But @upworthy provide access to a dataset of A/B tests from Upworthy, a clickbait media website that used A/B testing to optimize their content. @aboutupworthy provides more background information about Upworthy. And the datasets of A/B tests are available [here](https://osf.io/jd64p/).

We can look at what the dataset looks like, and get a sense for it by looking at the names and an extract. 

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

upworthy <- read_csv("https://osf.io/vy8mj/download")
```


```{r}
#| eval: true
#| include: false
#| warning: false
#| message: false

# INTERNAL

upworthy <- read_csv(here::here("dont_push/upworthy-archive-exploratory-packages-03.12.2020.csv"))
```


```{r}
#| message: false
#| warning: false

upworthy |> 
  names()

upworthy |> 
  head()
```

It is also useful to look at the documentation for the dataset. This describes the structure of the dataset, which is that there are packages within tests. A package is a collection of headlines and images that were shown randomly to different visitors to the website, as part of a test. A test can include many packages. Each row in the dataset is a package and the test that it is part of is specified by the 'clickability_test_id' column.

There are a variety of variables. We will focus on: 

- 'created_at', 
- 'clickability_test_id' so that we can create comparison groups, 
- 'headline', 
- 'impressions' which is the number of people that saw the package, and 
- 'clicks' which is the number that clicked on that package. 

Within each batch of tests, we are interested in the effect of the varied headlines on impressions and clicks.

```{r}
upworthy_restricted <- 
  upworthy |> 
  select(created_at, clickability_test_id, headline, impressions, clicks)

head(upworthy_restricted)
```

We will focus on the text contained in headlines, and look at whether headlines that asked a question got more clicks than those that did not. We want to remove the effect of different images and so will focus on those tests that have the same image. To identify whether a headline asks a question, we search for a question mark. Although there are more complicated constructions that we could use, this is enough to get started.

```{r}
upworthy_restricted <-
  upworthy_restricted |>
  mutate(asks_question = str_detect(string = headline, pattern = "\\?"))

upworthy_restricted |> 
  count(asks_question)
```

```{r}

upworthy_restricted <-
  upworthy_restricted |>
  mutate(asks_question = str_detect(string = headline, pattern = "\\?"))

upworthy_restricted |> 
  count(asks_question)
```

For every test, and for every picture, we want to know whether asking a question affected the number of clicks.

```{r}
to_question_or_not_to_question <- 
  upworthy_restricted |> 
  group_by(clickability_test_id, asks_question) |> 
  summarize(ave_clicks = mean(clicks)) |> 
  ungroup()

look_at_differences <- 
  to_question_or_not_to_question |> 
  pivot_wider(id_cols = clickability_test_id,
              names_from = asks_question,
              values_from = ave_clicks) |> 
  rename(ave_clicks_not_question = `FALSE`,
         ave_clicks_is_question = `TRUE`) |> 
  filter(!is.na(ave_clicks_not_question)) |>
  filter(!is.na(ave_clicks_is_question)) |> 
  mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question)

look_at_differences$difference_in_clicks |> mean()
```

We could also consider a cross-tab (@tbl-datasummaryupworthy).

```{r}
#| label: tbl-datasummaryupworthy
#| tbl-cap: "Difference between the average number of clicks"

library(modelsummary)

datasummary(ave_clicks ~ Mean * asks_question,
            data = to_question_or_not_to_question)

```

We find that in general, having a question in the headline may slightly decrease the number of clicks on a headline, although if there is an effect it does not appear to be very large (@fig-upworthy).

```{r}
#| fig-cap: "Comparison of the average number of clicks when a headline contains a question mark or not"
#| echo: false
#| label: fig-upworthy
#| message: false
#| warning: false

to_question_or_not_to_question |> 
  ggplot(aes(x = ave_clicks, 
             fill = asks_question)
         ) +
  geom_histogram() +
  labs(x = "Row number",
       y = "Average number of clicks",
       fill = "Has a question mark?") +
  theme_classic() 
```


## Surveys

### Designing surveys

Having decided what to measure, one common way to get values is to use a survey. Again, this is especially challenging, and there is an entire field, survey research, focused on this. While we are in the third era of survey research, characterized by low participation rates and increased alternatives [@Groves2011], the use of surveys remains popular and invaluable to hunt data. We now discuss designing surveys and introduce one way to implement them.

The survey form needs to be considered within the context of the broader research and with special concern for the respondent. The most important aspect is to test the survey before releasing it more broadly. If you do not have time, or budget, to test the survey, then do not bother doing the survey. 

Drawing on @surveydesign, all questions need to be relevant and able to be answered to by the respondent. The wording of the questions should be based on what the respondent would be comfortable with. The decision between different question types turns on minimizing error and the burden that we impose on the respondent. In general, if there are a small number of clear options then MCQ is appropriate. In that case, the responses should usually be mutually exclusive and collectively exhaustive. If they are not mutually exclusive, then this needs to be signaled in the text of the question. It is also important that units are specified, and that standard concepts are used, to the extent possible. 

Open text boxes may be appropriate if there are a lot of potential answers, although it is important to recognize that this will increase the time the respondent spends completing the survey, and increase the time it will take to analyze the answers. It is important that only one question is asked at a time and that all questions be asked in a neutral way that does not lead to one particular response.

All surveys need to have an introduction that specifies a title for the survey, who is conducting it, and their contact details, and the purpose. It should also include a statement about protecting confidentiality and ethics review board permission, if appropriate.

When doing surveys it is critical to ask the right person. For instance, @Lichand2022 consider child labor, the extent of which is typically based on surveys of parents. When questions were instead asked of children themselves, a considerable under-reporting by parents was found.

### Implementing surveys

There are many ways to implement surveys, and this choice really matters. For instance, there are dedicated survey platforms such as Survey Monkey and Qualtrics. In general, the focus of those platforms is on putting together the survey form and they expect that we already have contact details for the sample of interest. Some other platforms, such as Mechanical Turk and Prolific, focus on providing that audience, and we can then ask that audience to do more than just take a survey. When using platforms like those, and other providers, it is vital to be  aware of who is in the sample [@Levay2016; @whereyoursurveycomesfrom] While that is useful, it usually comes with higher costs. Finally, platforms such as Facebook also provide the ability to run a survey. One especially common approach, because it is free, is to use Google Forms.

To create a survey with Google Forms, sign into your Google Account, go to Google Drive, and then click 'New' then 'Google Form'. By default, the form is largely empty (@fig-googleformfirst), and we should add a title and description.

![The default view when a new Google Form is created contains many empty fields](figures/googleformfirst.png){#fig-googleformfirst width=75% fig-align="center"}

By default, a multiple-choice question is included, and we can update the content of this by clicking in the question field. Helpfully, there are often suggestions that can help provide the options. We can make the question required by toggling (@fig-googleformsecond).

![Updating the multiple-choice question that is included by default](figures/googleformsecond.png){#fig-googleformsecond width=75% fig-align="center"}

We can add another question, by clicking on the plus with a circle around it, and select different types of question, for instance, 'Short answer', 'Checkboxes', or 'Linear scale' (@fig-googleformthird). It can be especially useful to use 'Short answer' for aspect such as name and email address, checkboxes and linear scale to understand preferences.

![Different options for questions include short answer, checkboxes, and linear scale](figures/googleformthird.png){#fig-googleformthird width=75% fig-align="center"}

When we are happy with our survey, we make like to preview it ourselves, by clicking on the icon that looks like an eye. After checking it that way, we can click on 'Send'. Usually it is especially useful to use the second option, which is to send via a link, and it can be handy to shorten the URL (@fig-googleformfourth).

![There are a variety of ways to share the survey, and one helpful one is to get a link with a short URL](figures/googleformfourth.png){#fig-googleformfourth width=75% fig-align="center"}

After you share your survey, results will accrue in the 'Responses' tab and it can be especially useful to create a spreadsheet to view these responses, by clicking on the 'Sheets' icon. After you have collected enough responses then you can turn off 'Accepting responses' (@fig-googleformfifth).

![Responses show up alongside the survey and it can be helpful to add those to a separate spreadsheet](figures/googleformfifth.png){#fig-googleformfifth width=75% fig-align="center"}









## Exercises and tutorial


### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: *A political candidate is interested in how two polling values change over the course of an election campaign: approval rating, and vote-share. The two are measured as percentages, and are somewhat correlated. There tends to be large changes when there is a debate between candidates.* Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation.
3. *(Acquire)* Please describe a possible source of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched using the simulated data.
5. *(Communicate)* Please write two paragraphs about what you did.
6. In your own words, what is the fundamental problem of causal inference, being sure to include an example (write two or three paragraphs)?
7. In your own words, what is a counterfactual, being sure to include an example (write two or three paragraphs)?
8. In your own words, what is the role of randomization in constructing a counterfactual (write two or three paragraphs)?
9. What is external validity (pick one)?
    a. Findings from an experiment hold in that setting.
    b.  Findings from an experiment hold outside that setting.
    c. Findings from an experiment that has been repeated many times.
    d. Findings from an experiment for which code and data are available.
10. What is internal validity (pick one)?
    a.  Findings from an experiment hold in that setting.
    b. Findings from an experiment hold outside that setting.
    c. Findings from an experiment that has been repeated many times.
    d. Findings from an experiment for which code and data are available.
11. If we have a dataset named 'netflix_data', with the columns 'person' and 'tv_show' and 'hours', (person is a character class uniqueID for every person, tv_show is a character class name of a TV show, and hours is double expressing the number of hours that person watched that TV show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this:

```{r}
library(tidyverse)
netflix_data <- 
  tibble(person = c("Ian", "Ian", "Roger", "Roger", "Roger", 
                    "Patricia", "Patricia", "Helen"),
         tv_show = c("Broadchurch", "Duty-Shame", "Broadchurch", "Duty-Shame", 
                     "Shetland", "Broadchurch", "Shetland", "Duty-Shame"),
         hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)
         )
```

12. In the context of randomization, what does stratification mean to you (write a paragraph or two)?
13. How could you check that your randomization had been done appropriately (write two or three paragraphs)?
14. Identify three companies that conduct A/B testing commercially and write one paragraph for each of them about how they work and the trade-offs involved.
15. Pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency's Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why?
16. What is an estimate (pick one)?
    a. A rule for calculating an estimate of a given quantity based on observed data.
    b. The quantity of interest.
    c. The result.
    d. Unknown numbers that determine a statistical model.
17. What is an estimator (pick one)? 
    a. A rule for calculating an estimate of a given quantity based on observed data.
    b. The quantity of interest.
    c. The result.
    d. Unknown numbers that determine a statistical model.
18. What is an estimand (pick one)? 
    a. A rule for calculating an estimate of a given quantity based on observed data.
    b. The quantity of interest.
    c. The result.
    d. Unknown numbers that determine a statistical model.
19. What is a parameter (pick one)?
    a. A rule for calculating an estimate of a given quantity based on observed data.
    b. The quantity of interest.
    c. The result.
    d. Unknown numbers that determine a statistical model.
20. @ware1989investigating [p. 298] mentions 'a randomized play the winner design'. What is it?
21. @ware1989investigating [p. 299] mentions 'adaptive randomization'. What is it, in your own words?
22. @ware1989investigating [p. 299] mentions 'randomized-consent'. He continues that it was 'attractive in this setting because a standard approach to informed consent would require that parents of infants near death be approached to give informed consent for an invasive surgical procedure that would then, in some instances, not be administered. Those familiar with the agonizing experience of having a child in a neonatal intensive care unit can appreciate that the process of obtaining informed consent would be both frightening and stressful to parents'. To what extent do you agree with this position, especially given, as Ware (1989), p. 305, mentions 'the need to withhold information about the study from parents of infants receiving CMT'?
23. @ware1989investigating [p. 300] mentions 'equipoise'. In your own words, please define and discuss it, using an example from your own experience.




### Tutorial {.unnumbered}

Please build a website using `postcards` [@citepostcards]. Add Google Analytics. Deploy it using Netlify. Change some aspect of the website, add a different tracker, and push it to a new branch. Then use Netlify to conduct an A/B test. Write a one-to-two page paper about what you did and what you found.


### Paper {.unnumbered}

At about this point, Paper Three in [Appendix -@sec-papers] would be appropriate.
