---
engine: knitr
---

# Text as data {#sec-text-as-data}

**Required material**

- Read *Text as data: An overview*, [@benoit2020text]
- Read *The Naked Truth: How the names of 6,816 complexion products can reveal bias in beauty*, [@thenakedtruth]
- Read *Supervised Machine Learning for Text Analysis in R*, Chapters 2 "Tokenization", 3 "Stop words", 6 "Regression", and 7 "Classification", [@hvitfeldt2021supervised]

**Key concepts and skills**

- Understanding text as a source of data that we can analyze enables a large number of interesting questions to be considered.
- Text cleaning and preparation are especially critical because of the large number of possible outcomes. There are many decisions that need to be made at this stage, which have important effects later in the analysis.
- One way to consider a text dataset is to look at which words distinguish particular documents.
- Another is to consider which topics are contained in a document.

**Key packages and functions**

- Base R [@citeR]
  - `unlist()`
- `astrologer` [@astrologer]
	- `horoscopes`
- `beepr` [@beepr]
	- `beep()`
- `devtools` [@citeDevtools]
	- `install_github()`
- `fs` [@fs]
	- `dir_ls()`
- `quanteda` [@quanteda]
	- `char_wordstem()`
	- `corpus()`
	- `dfm()`
	- `dfm_remove()`
	- `dfm_trim()`
	- `dfm_wordstem()`
	- `labelTopics()`
	- `quanteda()`
	- `stopwords()`
	- `tokens()`
	- `tokens_compound()`
	- `tokens_ngrams()`
- `stm` [@stm]
	- `stm()`
- `tidytext` [@SilgeRobinson2016]
	- `bind_tf_idf()`
	- `unnest_tokens()`
- Core `tidyverse` [@tidyverse]
  - `stringr` [@citestringr]
	  - `str_replace_all()`
	  - `str_to_lower()`

## Introduction

Text is all around us. In many cases text is the earliest type of data that we are exposed to. Increases in computational power, the development of new methods, and the enormous availability of text, means that there has been a great deal of interest in using text as data. And they are an exciting option to have. For instance, text analysis of state-run newspapers in African countries can identify manipulation by governments [@Hassan2022]. The analysis of notes in Electronic Health Records (EHR) can improve the efficiency of disease prediction [@jessgronsbell]. And analysis of US congressional records indicates just how much women legislators are interrupted by men [@millersutherland2022].

Earlier approaches to the analysis of text tend to convert words into numbers, divorced of context. They could then be analyzed using traditional approaches, such as variants of logistic regression. More recent methods try to take advantage of the structure inherent in text, which can bring additional meaning. The difference is perhaps akin to a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, you can do more by knowing that a crocodile could eat you while a tree probably would not.

Text can be considered an unwieldy, but in general similar, version of the datasets that we have used throughout this book. The main difference is that we will typically begin with very wide data, insofar as often each variable is a word, or token more generally. Each entry is then often a count. We would then typically transform this into rather long data, with one variable of words and another of the counts. Considering text as data naturally requires some abstraction from its context. But it is important it is not entirely separated as this can perpetuate historical inequities. For instance, @koenecke2020 find that automated speech recognition systems perform much worse for Black compared with white speakers, and @davidson2019racial find that tweets that use Black American English, which is a specifically defined technical term, are classified at hate speech at higher rates than similar tweets in Standard American English, which again is a technical term.

In this chapter we cover a variety of approaches that enable us to consider text as data. One exciting aspect of text data is that it is typically not generated for the purposes of our analysis. The trade-off is that we typically must do a bunch more work to get it into a form that we can work with. And there are a lot of decisions to be made in the data cleaning and preparation stages.

The larger size of text datasets means that it is especially important to simulate, and start small, when it comes to their analysis. Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organized in your own mind. Nonetheless, it is an exciting area.

In this chapter we first consider preparing text datasets. We then consider logistic and lasso regression. We finally consider topic models and word embedding. 

## Text cleaning and preparation

Text modelling is an exciting area of research. But, and this is true more generally, the cleaning and preparation aspect is at least as difficult as the modelling. We will cover some essentials and provide a foundation that can be built on.

The first step is to get some data. We discussed data gathering in @sec-gather-data and mentioned in passing many sources including:

- Accessing the Twitter API using `rtweet` [@rtweet].
- Using *Inside Airbnb*, which provides text from reviews. 
- Getting the text from out-of-copyright books using `gutenbergr` [@gutenbergr].
- Scraping Wikipedia or other websites.

The workhorse packages that we need for text cleaning and preparation are `stringr` [@citestringr], which is part of the `tidyverse` [@tidyverse], and `quanteda` [@quanteda].

For illustrative purposes we construct a corpus of the first sentence, or two, from three books: "Beloved", "Don Quixote", and "Jane Eyre".

```{r}
#| message: false
#| warning: false

library(quanteda)
library(tidyverse)

don_quixote <-
  paste0(
    "In a village of La Mancha, the name of which I have no desire to ",
    "call to mind, there lived not long since one of those gentlemen ",
    "that keep a lance in the lance-rack, an old buckler, a lean hack, ",
    "and a greyhound for coursing."
  )

beloved <- "124 was spiteful. Full of Baby's venom."

jane_eyre <- "There was no possibility of taking a walk that day."

bookshelf <-
  tibble(
    book = c("Don Quixote", "Beloved", "Jane Eyre"),
    first_sentence = c(don_quixote, beloved, jane_eyre)
  )

bookshelf
```

We typically want to construct a document-feature matrix, which has documents in each observation, words in each column, and a count for each combination, along with associated metadata. For instance, if our corpus was the text from Airbnb reviews, then each document may be a review, and typical features could include "The", "Airbnb", "was", "great". Notice here that the sentence has been split into different words. We typically talk of "tokens" to generalize away from words, because of the variety of aspects we may be interested in, but words are commonly used.

```{r}
books_corpus <-
  corpus(bookshelf, docid_field = "book", text_field = "first_sentence")

books_corpus
```

We use the tokens in the corpus to construct a document-feature matrix using `dfm()` from `quanteda` [@quanteda].

```{r}
books_dfm <-
  books_corpus |>
  tokens() |>
  dfm()

books_dfm
```

While this is relatively straightforward, there are many decisions that will need to be made as part of this process, which we now consider. There is no definitive right or wrong answer. Instead, we make those decisions based on what we will be using the dataset for.

### Stop words

Stop words are words such as "the", "and", and "a". For a long time, such words were thought to not convey much meaning, and there was often a memory of computation constraint. A common step of preparing a text dataset was to remove them. We now know that stop words can have a great deal of meaning [@schofield2017]. The decision to remove them is a nuanced one that depends on circumstances.

We can get a list of stop words using `stopwords()` from `quanteda` [@quanteda], and then do a crude removal using `str_replace_all()`.

```{r}
stopwords(source = "snowball")[1:10]

stop_word_list <-
  paste(stopwords(source = "snowball"), collapse = " | ")

bookshelf |>
  mutate(extra_space = str_replace_all(
    string = first_sentence,
    pattern = "([ ,.])",  # Spaces and punctuation marks present in text
    replacement = " \\1"  # Add a space before any of the detected characters
  )) |>
  mutate(no_stops = str_replace_all(  # Remove stop words
    string = extra_space,
    pattern = stop_word_list,
    replacement = ""
  )) |>
  mutate(no_stops = str_squish(  # Remove extra spaces within string
    string = no_stops
  )) |>
  select(no_stops, first_sentence)
```

There are many different lists of stop words that have been put together by others. For instance, `stopwords()` can use lists including: "snowball", "stopwords-iso", "smart", "marimo", "ancient", and "nltk". More generally, if we decide to use stop words then we often need to augment such lists with project-specific words. We can do this by creating a count of individual words in the corpus, and then sorting by the most common and adding those to the stop words list as appropriate.

```{r}

stop_word_list_updated <-
  paste("village |", "spiteful |", "possibility |", 
        stop_word_list, collapse = " | ")

bookshelf |>
  mutate(extra_space = str_replace_all(  # Add double spaces
    string = first_sentence,
    pattern = " ",  
    replacement = "  " 
  )) |>
  mutate(no_stops = str_replace_all(  # Remove stop words
    string = extra_space,
    pattern = stop_word_list_updated,
    replacement = ""
  )) |>
  mutate(no_stops = str_squish(  # Remove extra spaces within string
    string = no_stops
  )) |>
  select(no_stops)

```

We can integrate the removal of stopwords into our construction of the DFM with `dfm_remove()`.

```{r}
books_dfm |>
  dfm_remove(stopwords(source = "snowball"))
```

When we remove stop words we artificially adjust our dataset. Sometimes there may be a good reason to do that. But it must not be done unthinkingly. For instance, in @sec-farm-data we discussed how sometimes datasets may need to be censored, truncated, or manipulated in other similar ways, to preserve the privacy of respondents. It is possible that the integration of the removal of stop words as a default step in natural language processing was due to computational power, which may have been more limited when these methods were developed. In any case, @jurafskymartin [p. 62] conclude that removing stop words does not improve performance for text classification. And @schofield2017 find that inference from topic models is not improved by the removal of anything other than the most frequent words. If stop words are to be removed, they recommend doing this after topics are constructed.

### Case, numbers, and punctuation

There are times when all we care about is the word, not the case nor punctuation. There are a variety of circumstances in which this may be appropriate. For instance, if the text corpus was particularly messy, or the existence of particular words was informative. We trade-off the loss of much information, for the benefit of making things more simple. We can convert to lower case with `str_to_lower()`, and use `str_replace_all()` to remove punctuation with "[:punct:]", and numbers with "[:digit:]".

```{r}
bookshelf |>
  mutate(lower_sentence = str_to_lower(string = first_sentence)) |>
  select(lower_sentence)
```


```{r}
bookshelf |>
  mutate(no_punctuation = str_replace_all(
    string = first_sentence,
    pattern = "[:punct:]",
    replacement = " "
  )) |>
  select(no_punctuation)
```


```{r}
bookshelf |>
  mutate(
    no_numbers = str_replace_all(
      string = first_sentence,
      pattern = "[:digit:]",
      replacement = " "
    )
  ) |>
  select(no_numbers)
```

As an aside, we can remove letters, numbers, and punctuation with "[:graph:]" in `str_replace_all()`. While this is rarely needed in textbook examples, it is especially useful with real datasets, because they will typically have a small number of unexpected symbols that we need to identify and then remove. We use it to remove everything that we are used to, leaving only that which we are not.

```{r}
bookshelf |>
  mutate(
    remove_obvious = str_replace_all(
      string = first_sentence,
      pattern = "[:graph:]",
      replacement = " "
    )
  ) |>
  select(remove_obvious)
```

More generally, we can use arguments in `tokens()` from `quanteda()` to do this. 

```{r}
books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE)
```

### Typos and uncommon words

Then we need to decide what to do about typos and other minor issues. Firstly, every real-world text has typos. Sometimes these should clearly be fixed. But if they are made in a systematic way, for instance, a certain writer always makes the same mistakes, then they would have value if we were interested in grouping by the writer. The use of OCR will introduce common issues as well, as was seen in @sec-gather-data. For instance, "the" is commonly incorrectly changed to "thc".

We could fix typos in the same way that we fixed stop words, i.e. with lists of corrections. When it comes to uncommon words, we can build this into our document-feature matrix creation with `dfm_trim()`. For instance we could use "min_termfreq = 2" to remove any word that does not occur at least twice, or "min_docfreq = 0.05" to remove any word that is not in at least 5 per cent of documents or "max_docfreq = 0.90" to remove any word that is in at least 90 per cent of documents.

```{r}
books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE) |>
  dfm(tolower = TRUE) |>
  dfm_trim(min_termfreq = 2)
```

### Tuples

A tuple is an ordered list of elements, and in the context of text, it is a series of words. If it is two words then we term this a "bi-gram", three words is a "tri-gram", etc. These are an issue when it comes to text cleaning and preparation because we typically need to separate terms based on a space, and this would result in inappropriate separation. 

This is a clear issue when it comes to place names. For instance, consider "British Columbia", "New Hampshire", "United Kingdom", and "Port Hedland". One way forward is to create a list of such places and then use `str_replace_all()` to add an underbar, for instance, "British_Columbia", "New_Hampshire", "United_Kingdom", and "Port_Hedland". Another option, is to use `tokens_compound()` from `quanteda`.

```{r}
some_places <- c("British Columbia", "New Hampshire", "United Kingdom", "Port Hedland")
a_sentence <- c("Vancouver is in British Columbia and New Hampshire is important for presidential hopefuls")

tokens(a_sentence) |>
  tokens_compound(pattern = phrase(some_places))
```

In that case, we knew what the tuples were. But it might be that we were not sure what the common tuples were in the corpus. We could then use `tokens_ngrams()` to identify them. We could ask for, say, all bi-grams in an excerpt from the book "Don Quixote".

```{r}
#| eval: false
#| echo: true

library(gutenbergr)

don_quixote <- 
  gutenberg_download(
    gutenberg_id = 996) |>
  filter(text != "") |>
  slice_sample(n = 1000)

write_csv(don_quixote, "books-don_quixote.csv")
```

```{r}
#| eval: false
#| echo: false

# INTERNAL

write_csv(don_quixote, "inputs/data/books-don_quixote.csv")
```



```{r}
#| eval: false
#| echo: true

don_quixote <- read_csv(
  "books-don_quixote.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)
```

```{r}
#| eval: true
#| echo: false

# INTERNAL

don_quixote <- read_csv(
  "inputs/data/books-don_quixote.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)
```


```{r}
don_q_text <- tibble(
  book = "Don Quixote",
  text = paste(don_quixote$text, collapse = " ") |>
    str_replace_all(pattern = "[:punct:]",
                    replacement = " ") |>
    str_replace_all(# Add double spaces
      pattern = " ",
      replacement = "  ") |>
    str_replace_all(# Remove stop words
      pattern = stop_word_list,
      replacement = "") |>
    str_squish()
)

don_q_corpus <-
  corpus(don_q_text, docid_field = "book", text_field = "text")

ngrams <- tokens_ngrams(tokens(don_q_corpus), n = 2)

ngram_counts <-
  tibble(ngrams = unlist(ngrams)) |>
  count(ngrams, sort = TRUE)

head(ngram_counts)
```

Having identifying some common bi-grams, we could add them to the list to be changed. This example includes names like Don Quixote and Sancho Panza which would need to remain together for analysis.

### Stemming and lemmatizing

Stemming and lemmatizing words is another common approach for reducing the dimensionality of a text dataset. Stemming means to remove the last part of the word, in the expectation that this will result in more general words. For instance, "Canadians", "Canadian", and "Canada" all stem to "Canad". Lemmatizing is similar, but is more involved. It means that changing words, not just on their spelling, but on their canonical form [@textasdata, p. 54]. For instance, "Canadians", "Canadian", "Canucks", and "Canuck", may all be changed to "Canada".

We can do this with `dfm_wordstem()`.

```{r}

char_wordstem(c("Canadians", "Canadian", "Canada"))

books_corpus |>
  tokens(remove_numbers = TRUE, remove_punct = TRUE) |>
  dfm(tolower = TRUE) |>
  dfm_wordstem()
```


Again, while this is a common step in using text as data, @schofield2017understanding find that in the context of LDA, which we cover later, stemming has little effect and there is little need to do it.


### Duplication

Duplication is a major concern with text datasets because of their size. @bandy2021addressing showed that around 30 per cent of the data were inappropriately duplicated in a large text dataset commonly used in computer science. And @schofield2017quantifying show that this is a major concern and could substantially affect results. However, it can be a subtle and difficult to diagnose problem. For instance, in @sec-its-just-a-linear-model when we considered counts of page numbers for various authors in the context of Poisson regression, we could easily have accidentally included each Shakespeare entry twice because not only are there entries for each play, but also many anthologies that contained all of them. Careful consideration of our dataset identified the issue, but that would be difficult at scale.



## TF-IDF

We now use `astrologer` [@astrologer], which is a dataset of horoscopes to explore a real dataset. This package is not on CRAN, so we need to install it using `devtools` [@citeDevtools].

```{r}
#| echo: true
#| eval: false
library(devtools)

install_github("sharlagelfand/astrologer")
```

We can then access the "horoscopes" dataset.

```{r}
#| warning: false
#| message: false

library(astrologer)
library(tidyverse)

horoscopes
```

There are four variables: "startdate", "zodiacsign", "horoscope", and "url" (note that URL is out-of-date because the website has been updated, for instance, the first one refers to [here](https://chaninicholas.com/horoscopes-week-january-5th/)). We are interested in the words that are used to distinguish the horoscope of each zodiac sign.

```{r}
horoscopes |>
  count(zodiacsign)
```

We can see that there are 106 horoscopes for each zodiac sign. In this example we first tokenize by word, and then create counts based on zodiac sign only, not date. We use `tidytext` [@SilgeRobinson2016], as it is used extensively in @hvitfeldt2021supervised.

```{r}
library(tidytext)

horoscopes_by_word <-
  horoscopes |>
  select(-startdate, -url) |>
  unnest_tokens(
    output = word,
    input = horoscope,
    token = "words"
  )

horoscopes_counts_by_word <-
  horoscopes_by_word |>
  count(zodiacsign, word, sort = TRUE)

head(horoscopes_counts_by_word)
```

We can see that the most popular words appear to be similar for the different zodiacs. At this point, we could use the data in a variety of ways. We might be interested to know which words characterize each group---that is to say, which words are commonly used only in each group. We can do that by first looking at a word's term frequency (TF), which is how many times a word is used in the horoscopes for each zodiac sign. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency (IDF) in which we "penalize" words that occur in the horoscopes for many zodiac signs. A word that occurs in the horoscopes of many zodiac signs would have a lower IDF than a word that only occurs in the horoscopes of one. The term frequencyâ€“inverse document frequency (tf-idf) is then the product of these.

We can create this value using `bind_tf_idf()` from `tidytext`. It will create new variables for each of these measures.

```{r}
horoscopes_counts_by_word_tf_idf <-
  horoscopes_counts_by_word |>
  bind_tf_idf(
    term = word,
    document = zodiacsign,
    n = n
  ) |>
  arrange(-tf_idf)

horoscopes_counts_by_word_tf_idf
```

In @tbl-zodiac we look at the words that distinguish the horoscopes of each zodiac sign. The first thing to notice is that some of them have their own zodiac sign. On the one hand, there is an argument for removing this, but on the other hand, the fact that it does not happen for all of them is perhaps informative of the nature of the horoscopes for each sign.

```{r}
#| label: tbl-zodiac
#| tbl-cap: "Most common words in horoscopes that are unique to a particular Zodiac sign"

horoscopes_counts_by_word_tf_idf |>
  group_by(zodiacsign) |>
  slice(1:10) |>
  select(zodiacsign, word) |>
  group_by(zodiacsign) |>
  summarise(all = paste0(word, collapse = "; ")) |>
  knitr::kable(
    col.names = c(
      "Zodiac sign",
      "Most common words unique to that sign"
    ),
    booktabs = TRUE
  )
```


<!-- ## Lasso regression -->

<!-- One of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use a variant of logistic regression, along with text inputs, to forecast. Inspired by @silge2018 we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, we could imagine many real-world applications. For instance, we may be interested in whether some text was likely written by a bot or a human. -->


<!-- First we need to get some data. We use books from Project Gutenberg using `gutenberg_download()` from `gutenbergr` [@gutenbergr]. We consider *Jane Eyre* [@janeeyre] and *Alice's Adventures in Wonderland* [@citealice]. -->

<!-- ```{r} -->
<!-- #| include: true -->
<!-- #| message: false -->
<!-- #| warning: false -->
<!-- #| eval: false -->

<!-- library(gutenbergr) -->
<!-- library(tidyverse) -->

<!-- # The books that we are interested in have the keys of 1260 and 11, respectively. -->
<!-- alice_and_jane <- -->
<!--   gutenberg_download( -->
<!--     gutenberg_id = c(1260, 11), -->
<!--     meta_fields = "title" -->
<!--   ) -->

<!-- write_csv(alice_and_jane, "alice_and_jane.csv") -->

<!-- head(alice_and_jane) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| include: false -->
<!-- #| eval: true -->
<!-- #| warning: false -->
<!-- #| message: false -->

<!-- library(gutenbergr) -->
<!-- library(tidyverse) -->

<!-- alice_and_jane <- read_csv("inputs/books/alice_and_jane.csv") -->

<!-- head(alice_and_jane) -->
<!-- ``` -->

<!-- One of the great things about this is that the dataset is a tibble. So we can work with all our familiar skills. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other.  -->

<!-- By looking at the number of lines in each, it looks like *Jane Eyre* is much longer than *Alice in Wonderland*. We start by getting rid of blank lines using `remove_empty()` from `janitor` [@janitor]. -->

<!-- ```{r} -->
<!-- library(janitor) -->

<!-- alice_and_jane <- -->
<!--   alice_and_jane |> -->
<!--   mutate(blank_line = if_else(text == "", 1, 0)) |> -->
<!--   filter(blank_line == 0) |> -->
<!--   select(-blank_line) -->

<!-- table(alice_and_jane$title) -->
<!-- ``` -->

<!-- There is still an overwhelming amount of *Jane Eyre*, compared with *Alice in Wonderland*, so we sample from *Jane Eyre* to make it more equal. -->

<!-- ```{r} -->
<!-- set.seed(853) -->

<!-- alice_and_jane$rows <- c(1:nrow(alice_and_jane)) -->
<!-- sample_from_me <- alice_and_jane |> filter(title == "Jane Eyre: An Autobiography") -->
<!-- keep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE) -->

<!-- alice_and_jane <- -->
<!--   alice_and_jane |> -->
<!--   filter(title == "Alice's Adventures in Wonderland" | rows %in% keep_me) |> -->
<!--   select(-rows) -->

<!-- table(alice_and_jane$title) -->
<!-- ``` -->

<!-- There are a variety of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless we continue and add a counter with the line number for each book. -->

<!-- ```{r} -->
<!-- alice_and_jane <- -->
<!--   alice_and_jane |> -->
<!--   group_by(title) |> -->
<!--   mutate(line_number = paste(gutenberg_id, row_number(), sep = "_")) |> -->
<!--   ungroup() -->
<!-- ``` -->

<!-- We now want to unnest the tokes. We use `unnest_tokens()` from `tidytext` [@SilgeRobinson2016]. -->

<!-- ```{r} -->
<!-- #| message: false -->
<!-- #| warning: false -->

<!-- library(tidytext) -->

<!-- alice_and_jane_by_word <- -->
<!--   alice_and_jane |> -->
<!--   unnest_tokens(word, text) |> -->
<!--   group_by(word) |> -->
<!--   filter(n() > 10) |> -->
<!--   ungroup() -->
<!-- ``` -->

<!-- We remove any word that was not used more than 10 times. Nonetheless we still have more than 500 unique words. (If we did not require that the word be used by the author at least 10 times then we end up with more than 6,000 words.) -->

<!-- The reason this is relevant is because these are our independent variables. So where we may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this. -->

<!-- However, as mentioned before, we are going to have some rows that essentially just had one word. And so we filter for that also, which ensures that the model will have at least some words to work with. -->

<!-- ```{r} -->
<!-- alice_and_jane_by_word <- -->
<!--   alice_and_jane_by_word |> -->
<!--   group_by(title, line_number) |> -->
<!--   mutate(number_of_words_in_line = n()) |> -->
<!--   ungroup() |> -->
<!--   filter(number_of_words_in_line > 2) |> -->
<!--   select(-number_of_words_in_line) -->
<!-- ``` -->

<!-- We create a test/training split, and load in `tidymodels`. -->

<!-- ```{r} -->
<!-- #| message: false -->
<!-- #| warning: false -->

<!-- library(tidymodels) -->

<!-- set.seed(853) -->

<!-- alice_and_jane_by_word_split <- -->
<!--   alice_and_jane_by_word |> -->
<!--   select(title, line_number) |> -->
<!--   distinct() |> -->
<!--   initial_split(prop = 3 / 4, strata = title) -->
<!-- ``` -->

<!-- Then we can use `cast_dtm()` to create a document-term matrix. This provides a count of how many times each word is used. -->

<!-- ```{r} -->
<!-- alice_and_jane_dtm_training <- -->
<!--   alice_and_jane_by_word |> -->
<!--   count(line_number, word) |> -->
<!--   inner_join(training(alice_and_jane_by_word_split) |> select(line_number)) |> -->
<!--   cast_dtm(term = word, document = line_number, value = n) -->

<!-- dim(alice_and_jane_dtm_training) -->
<!-- ``` -->

<!-- So we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre. -->

<!-- ```{r} -->
<!-- response <- -->
<!--   data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) |> -->
<!--   separate(id, into = c("book", "line", sep = "_")) |> -->
<!--   mutate(is_alice = if_else(book == 11, 1, 0)) -->

<!-- predictor <- alice_and_jane_dtm_training[] |> as.matrix() -->
<!-- ``` -->


<!-- Now we can run our model. -->

<!-- ```{r} -->
<!-- #| eval: false -->

<!-- library(glmnet) -->

<!-- model <- cv.glmnet( -->
<!--   x = predictor, -->
<!--   y = response$is_alice, -->
<!--   family = "binomial", -->
<!--   keep = TRUE -->
<!-- ) -->

<!-- save(model, file = "alice_vs_jane.rda") -->
<!-- ``` -->

<!-- ```{r}  -->
<!-- #| echo: false -->

<!-- load("outputs/model/alice_vs_jane.rda") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library(glmnet) -->
<!-- library(broom) -->

<!-- coefs <- model$glmnet.fit |> -->
<!--   tidy() |> -->
<!--   filter(lambda == model$lambda.1se) -->

<!-- coefs |> head() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- coefs |> -->
<!--   group_by(estimate > 0) |> -->
<!--   top_n(10, abs(estimate)) |> -->
<!--   ungroup() |> -->
<!--   ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) + -->
<!--   geom_col(alpha = 0.8, show.legend = FALSE) + -->
<!--   coord_flip() + -->
<!--   theme_minimal() + -->
<!--   labs( -->
<!--     x = "Coefficient", -->
<!--     y = "Word" -->
<!--   ) + -->
<!--   scale_fill_brewer(palette = "Set1") -->
<!-- ``` -->

<!-- Perhaps unsurprisingly, if a line mentions Alice then it is likely to be a Alice in Wonderland and if it mention Jane then it is likely to be Jane Eyre. -->











## Topic models

Sometimes we have a statement, and we want to know what it is about. Sometimes this will be easy, but we do not always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of @Blei2003latent, as implemented by `stm` [@stm].

The key assumption behind the LDA method is that each statement, "a document", is made by a person who decides the topics they would like to talk about in that document, and who then chooses words, "terms", that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified *ex ante*; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics.

LDA considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (@fig-topicsoverdocuments).

```{r}
#| echo: false
#| fig-cap: "Probability distributions over topics"
#| label: fig-topicsoverdocuments
#| layout-ncol: 2
#| fig-subcap: ["Distribution for Document 1", "Distribution for Document 2"]

document_1 <- tibble(
  Topics = c(
    "topic 1",
    "topic 2",
    "topic 3",
    "topic 4",
    "topic 5"
  ),
  Probability = c(0.40, 0.40, 0.1, 0.05, 0.05)
)

document_2 <- tibble(
  Topics = c(
    "topic 1",
    "topic 2",
    "topic 3",
    "topic 4",
    "topic 5"
  ),
  Probability = c(0.01, 0.04, 0.35, 0.20, 0.4)
)

ggplot(document_1, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 1") +
  coord_cartesian(ylim = c(0, 0.4))

ggplot(document_2, aes(Topics, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over topics for Document 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (@fig-topicsoverterms).

```{r}
#| echo: false
#| fig-cap: "Probability distributions over terms"
#| label: fig-topicsoverterms
#| layout-ncol: 2
#| fig-subcap: ["Distribution for Topic 1", "Distribution for Topic 2"]

topic_1 <- tibble(
  Terms = c(
    "immigration",
    "race",
    "influx",
    "loans",
    "wealth",
    "saving",
    "chinese",
    "france",
    "british",
    "english"
  ),
  Probability = c(0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.2, 0.15, 0.4, 0.2)
)

topic_2 <- tibble(
  Terms = c(
    "immigration",
    "race",
    "influx",
    "loans",
    "wealth",
    "saving",
    "chinese",
    "france",
    "british",
    "english"
  ),
  Probability = c(0.0142, 0.0142, 0.0142, 0.25, 0.35, 0.30, 0.0142, 0.0142, 0.0142, 0.0142)
)

ggplot(topic_1, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 1") +
  coord_cartesian(ylim = c(0, 0.4))
ggplot(topic_2, aes(Terms, Probability)) +
  geom_point() +
  theme_classic() +
  labs(title = "Distribution over terms for Topic 2") +
  coord_cartesian(ylim = c(0, 0.4))
```

<!-- Following @BleiLafferty2009, @blei2012 and @GriffithsSteyvers2004, the process by which a document is generated is more formally considered to be: -->

<!-- 1. There are $1, 2, \dots, k, \dots, K$ topics and the vocabulary consists of $1, 2, \dots, V$ terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the $k$th topic is $\beta_k$. Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter $0<\eta<1$ is used: $\beta_k \sim \mbox{Dirichlet}(\eta)$.[^Dirichletfootnote] Strictly, $\eta$ is actually a vector of hyperparameters, one for each $K$, but in practice they all tend to be the same value. -->
<!-- 2. Decide the topics that each document will cover by randomly drawing distributions over the $K$ topics for each of the $1, 2, \dots, d, \dots, D$ documents. The topic distributions for the $d$th document are $\theta_d$, and $\theta_{d,k}$ is the topic distribution for topic $k$ in document $d$. Again, the Dirichlet distribution with the hyperparameter $0<\alpha<1$ is used here because usually a document would only cover a handful of topics: $\theta_d \sim \mbox{Dirichlet}(\alpha)$. Again, strictly $\alpha$ is vector of length $K$ of hyperparameters, but in practice each is usually the same value. -->
<!-- 3. If there are $1, 2, \dots, n, \dots, N$ terms in the $d$th document, then to choose the $n$th term, $w_{d, n}$: -->
<!--     a. Randomly choose a topic for that term $n$, in that document $d$, $z_{d,n}$, from the multinomial distribution over topics in that document, $z_{d,n} \sim \mbox{Multinomial}(\theta_d)$.  -->
<!--     b. Randomly choose a term from the relevant multinomial distribution over the terms for that topic, $w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})$. -->
    
By way of background, the Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, $\eta=1$, it is equivalent to a uniform distribution. If $\eta<1$, then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as $\eta$ decreases. A hyperparameter, in this usage, is a parameter of a prior distribution.

<!-- Given this set-up, the joint distribution for the variables is [@blei2012, p.6]: -->
<!-- $$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).$$ -->

<!-- Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over $\beta_{1:K}$ and $\theta_{1:D}$, given $w_{1:D, 1:N}$. This is intractable directly, but can be approximated [@GriffithsSteyvers2004; @blei2012]. -->

After the documents are created, they are all that we can analyze. The term usage in each document is observed, but the topics are hidden, or "latent". We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of @fig-topicsoverdocuments or @fig-topicsoverterms. In a sense we are trying to reverse the document generation process---we have the terms, and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics [@SteyversGriffiths2006]. The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. 

<!-- That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (@blei2012, p.7):  -->
<!-- $$p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.$$ -->

The initial practical step when implementing LDA given a corpus of documents is usually to remove stop words. Although, as mentioned earlier, this is not necessary, and may be better done after the groups are created. We often also remove punctuation and capitalization. We then construct our document-feature matrix using `dfm()` from `quanteda` [@quanteda].

After the dataset is ready, `stm` [@stm] can be used to implement LDA and approximate the posterior. 
<!-- It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following @SteyversGriffiths2006 and @Darling2011, the Gibbs sampling  -->
The process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors.
<!-- with $\alpha = \frac{50}{K}$ and $\eta = 0.1$  (@SteyversGriffiths2006 recommends $\eta = 0.01$), where $\alpha$ refers to the distribution over topics and $\eta$ refers to the distribution over terms (@Grun2011, p.7).  -->
It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given.
[@Grun2011, p.6]:
<!-- $$p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} $$ -->
<!-- where $z'_{d, n}$ refers to all other topic assignments; $\lambda'_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$; $\lambda'_{.\rightarrow k}$ is a count of how many other times that any term has been assigned to topic $k$; $\lambda'^{(d)}_{n\rightarrow k}$ is a count of how many other times that term has been assigned to topic $k$ in that particular document; and $\lambda'^{(d)}_{-i}$ is a count of how many other times that term has been assigned in that document.  -->
Once this has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document [@SteyversGriffiths2006]. The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate. 

The choice of the number of topics, *k*, affects the results, and must be specified *a priori*. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for *k* and then picking an appropriate value that performs well.

One weakness of the LDA method is that it considers a "bag of words" where the order of those words does not matter [@blei2012]. It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. 

### What is talked about in Canadian parliament?

Following the example of the British, the written record of what is said in the Canadian parliament is called "Hansard". It is not completely verbatim, but is very close. It is available in CSV format from [lipad](https://www.lipad.ca), which was constructed by @BeelenEtc2017.

We are interested in what was talked about in the Canadian parliament in 2018. To get started we can download the entire corpus from [here](https://www.lipad.ca/data/), and then discard all of the years apart from 2018. If the datasets are in a folder called "2018", we can use `read_csv()` to read and combine all the CSVs.

```{r}
#| echo: true
#| eval: false

library(fs)
library(tidyverse)

files_of_interest <-
  dir_ls(
    path = "2018/",
    glob = "*.csv",
    recurse = 2
  )

hansard_canada_2018 <-
  read_csv(
    files_of_interest,
    col_types = cols(
      basepk = col_integer(),
      hid = col_character(),
      speechdate = col_date(),
      pid = col_character(),
      opid = col_integer(),
      speakeroldname = col_character(),
      speakerposition = col_character(),
      maintopic = col_character(),
      subtopic = col_character(),
      subsubtopic = col_character(),
      speechtext = col_character(),
      speakerparty = col_character(),
      speakerriding = col_character(),
      speakername = col_character(),
      speakerurl = col_character()
    ),
    col_select = c(
      basepk,
      speechdate,
      speechtext,
      speakername,
      speakerparty,
      speakerriding
    )
  ) |>
  filter(!is.na(speakername))

hansard_canada_2018
```

```{r}
#| echo: false
#| eval: true

library(fs)
library(tidyverse)

files_of_interest <-
  dir_ls(
    path = "inputs/data/2018/",
    glob = "*.csv",
    recurse = 2
  )

hansard_canada_2018 <-
  read_csv(
    files_of_interest,
    col_types = cols(
      basepk = col_integer(),
      hid = col_character(),
      speechdate = col_date(),
      pid = col_character(),
      opid = col_integer(),
      speakeroldname = col_character(),
      speakerposition = col_character(),
      maintopic = col_character(),
      subtopic = col_character(),
      subsubtopic = col_character(),
      speechtext = col_character(),
      speakerparty = col_character(),
      speakerriding = col_character(),
      speakername = col_character(),
      speakerurl = col_character()
    ),
    col_select = c(
      basepk,
      speechdate,
      speechtext,
      speakername,
      speakerparty,
      speakerriding
    )
  ) |>
  filter(!is.na(speakername))

hansard_canada_2018
```

The use of `filter()` at the end is needed because sometime aspects such as "directions" and similar non-speech aspects, are included in the Hansard. For instance, if we do not include that `filter()` then the first line is "The House resumed from November 9, 2017, consideration of the motion." We can then construct a corpus.

```{r}
#| warning: false
#| message: false

hansard_canada_2018_corpus <-
  corpus(hansard_canada_2018, docid_field = "basepk", text_field = "speechtext")

hansard_canada_2018_corpus
```

We use the tokens in the corpus to construct a document-feature matrix. To make our life a little easier, computationally, we remove any word that does not occur at least twice, and any word that does not occur in at least two documents.

```{r}
hansard_dfm <-
  hansard_canada_2018_corpus |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE
  ) |>
  dfm() |>
  dfm_trim(min_termfreq = 2, min_docfreq = 2) |>
  dfm_remove(stopwords(source = "snowball"))

hansard_dfm
```

At this point we can use `stm()` from `stm` [@stm] to implement an LDA model. We need to specify a document-feature matrix and the number of topics. Topic models are essentially just summaries. Instead of a document becoming a collection of words, they become a collection of topics with some probability associated with each topic. But because it is just providing a collection of words that tend to be used at similar times, rather than actual underlying meaning, we need to specify the number of topics that we are interested in. This decision will have a big impact, and it is important to consider a few different numbers.

```{r}
#| echo: true
#| eval: false

library(stm)
library(beepr)

hansard_topics <- stm(documents = hansard_dfm, K = 10)

beep()

write_rds(
  hansard_topics,
  file = "hansard_topics.rda"
)
```

```{r}
#| echo: false
#| eval: false

# INTERNAL
library(beepr)
library(stm)

hansard_topics <- stm(documents = hansard_dfm, K = 10)

beep()

write_rds(
  hansard_topics,
  file = "outputs/hansard_topics.rda"
)
```

This will take some time, likely 15-30 min, so it is useful to save the model when it is done using `write_rds()`, and use `beep` to get a notification when it is done. We could then read the results back in with `read_rds()`.

```{r}
#| echo: true
#| eval: false

hansard_topics <- read_rds(
  file = "hansard_topics.rda"
)
```

```{r}
#| echo: false
#| eval: true

hansard_topics <- read_rds(
  file = "outputs/hansard_topics.rda"
)
```

We can look at the words in each topic with `labelTopics()`.

```{r}
#| message: false

library(stm)
labelTopics(hansard_topics)
```










<!-- ## Word embeddings -->

<!-- Add Python here? -->




<!-- ## Simulated example - categorizing spending by merchant -->

<!-- *Thank you to Josh Harris for discussions that led to this example. The code draws on Alexander and Leslie, 2019.* -->

<!-- To begin with, let's pretend that we work for KOHO - a Toronto-based fintech. One important feature of the KOHO app is that it categorizes your spending. For instance, buying a double-double from Tim Hortons could be categorized into "Eating out". This works because each merchant has a unique name, however that uniqueness means that the Tim Hortons at Sid Smith has a slightly different name to the Tim Hortons at Queen and John. Despite this slightly different name, we would like for them both to be classified as "Eating out". -->

<!-- To get started, let's generate some data: -->

<!-- ```{r} -->
<!-- library(glmnet) -->
<!-- library(tidytext) -->
<!-- library(tidyverse) -->

<!-- size_of_labelled_data <- 1000 -->

<!-- # List of retailers in Toronto -->
<!-- list_of_retailers <- c("Tim Hortons", "Hudsons Bay", "Loblaws", "Shoppers Drug Mart", "Canadian Tire") -->

<!-- # List of roads in Toronto -->
<!-- # This is from https://en.wikipedia.org/wiki/List_of_north%E2%80%93south_roads_in_Toronto and https://en.wikipedia.org/wiki/List_of_east%E2%80%93west_roads_in_Toronto. -->
<!-- list_of_roads <- c("Adelaide St", "Allen Rd", "Annette St", "Ave Rd", "Bathurst St", "Bay St", "Bayview Ave", "Beare Rd", "Bellamy Rd", "Beverley St", "Birchmount Rd", "Black Creek Dr", "Bloor St", "Blythwood Rd", "Bond St", "Brimley Rd", "BRdview Ave", "Browns Line", "Burnhamthorpe Rd", "Caledonia Rd", "Carlingview Dr", "Carlton St", "Centennial Rd", "Christie St", "Church St", "College St", "Conlins Rd", "Coxwell Ave", "Cummer Ave", "Danforth Ave", "Davenport Rd", "Davisville Ave", "Dixon Rd", "Don Mills Rd", "Don Valley Parkway", "Donlands Ave", "Draper St", "Dufferin St", "Dundas St", "Dupont St", "East Ave", "Eastern Ave", "Eglinton Ave", "Ellesmere Rd", "Ellis Ave", "Finch Ave", "Front St", "Galloway Rd", "Gardiner Expressway", "Gerrard St", "Glencairn Ave", "Greenwood Ave", "Harbord St", "Highway 27", "Highway 401", "Highway 404", "Highway 427", "Islington Ave", "Jameson Ave", "Jane St", "Jarvis St", "John St", "Keele St", "Kennedy Rd", "King St", "Kipling Ave", "Laird Dr", "Lake Shore Boulevard", "Lansdowne Ave", "Lawrence Ave", "Leader Lane", "Leslie St", "Main St", "Manse Rd", "Markham Rd", "Marlee Ave", "Martin Grove Rd", "McCowan Rd", "Meadowvale Rd", "Middlefield Rd", "Midland Ave", "Mill St", "Millwood Rd", "Morningside Ave", "Morrish Rd", "Mount Pleasant Rd", "Neilson Rd", "Oakwood Ave", "OConnor Dr", "Ontario Highway 409", "Orfus Rd", "Orton Park Rd", "Ossington Ave", "Pape Ave", "Parkside Dr", "Parliament St", "Pharmacy Ave", "Port Union Rd", "Queen St", "Queens Park", "Queens Quay", "Rathburn Rd", "Rees St", "Reesor Rd", "Reggae Lane", "Renforth Dr", "Richmond St", "Rogers Rd", "Roncesvalles Ave", "Roselawn Ave", "Royal York Rd", "Runnymede Rd", "Scarborough Golf Club Rd", "Scarborough-Pickering Townline", "Scarlett Rd", "Sewells Rd", "Sheppard Ave", "Sherbourne St", "Simcoe St", "Spadina Ave", "St Clair Ave", "St George St", "Steeles Ave", "The Queensway", "University Ave", "Victoria Park Ave", "Warden Ave", "Wellesley St", "Wellington St", "Weston Rd", "Willowdale Ave", "Wilson Ave", "Woodbine Ave", "Yonge St", "York Mills Rd", "York St") -->

<!-- example_spending_data <- -->
<!--   tibble( -->
<!--     merchant_actual = sample(list_of_retailers, -->
<!--                              size = size_of_labelled_data, -->
<!--                              replace = TRUE), -->
<!--     merchant_unique_name_noise = sample(list_of_roads, -->
<!--                                         size = size_of_labelled_data, -->
<!--                                         replace = TRUE), -->
<!--     merchant_unique_name = paste(merchant_actual, merchant_unique_name_noise, sep = " ") -->
<!--     ) |> -->
<!--   select(-merchant_unique_name_noise) -->

<!-- head(example_spending_data) -->
<!-- ``` -->

<!-- We could pretend that we have these 1,000 data points of the unique names and that we labelled the actual merchant by hand so that we are sure. Let's use this dataset to train a model. We'll then use that trained model on a new dataset. -->

<!-- First make a separate line for every word. -->

<!-- ```{r} -->
<!-- all_words <- -->
<!--   example_spending_data |> -->
<!--   unnest_tokens(word, merchant_unique_name) -->

<!-- head(all_words) -->
<!-- ``` -->

<!-- Now create a document-term matrix. -->

<!-- ```{r} -->
<!-- example_dtm <- -->
<!--   all_words |> -->
<!--   count(merchant_actual, word, sort = TRUE) |> -->
<!--   cast_dtm(merchant_actual, word, n) -->
<!-- ``` -->



<!-- ```{r} -->
<!-- predictor <- example_dtm |> as.matrix() -->
<!-- response <- dimnames(example_dtm)[[1]] -->
<!-- ``` -->

<!-- Now we can model it. -->

<!-- ```{r} -->
<!-- # names_model <- cv.glmnet(predictor, -->
<!-- #                          response, -->
<!-- #                          family = "multinomial", -->
<!-- #                          alpha = 0.9) -->
<!-- ``` -->



<!-- ### Google Colab -->

<!-- Google Colab is similar to RStudio Cloud, in that it is set-up to allow us to just log in and get started. In this case, we need a Google account. While we can use R on Google Colab, it is designed for Python. Advantages over RStudio Cloud include the ability to use GPUs, not just CPUs. -->

<!-- To get started, while logged into Google, go to [Colab](colab.research.google.com) and select "New notebook". -->

<!-- To get started you need to tell Google Colab that you want to use R. You can do this by using this: https://colab.research.google.com/notebook#create=true&language=r. -->

<!-- At this point you have a Jupyter notebook open that will run R. (But it is not a Quarto document.) You can install packages as normal, e.g. `install.packages("tidyverse")`, and then call the package e.g. `library(tidyverse)`. -->

<!-- Google Colab is a good option if you have a good reason for using the broader capabilities that it has. If you want to go deeper into that then the Morris reading has a bunch of options that you can explore, but as Morris puts it "Colab is a gateway drug - for large-scale processing pipelines you'll need to move up to Google Cloud Platform or one of its competitors AWS, Azure, etc." and that is what we do now. -->




## Exercises and tutorial

### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: *You run a news website and are trying to understand whether to allow anonymous comments. You decide to do an A/B test, where we keep everything the same, but only allow anonymous comments on one version of the site. All you will have to decide is the text data that you obtain from the test.* Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation.
3. *(Acquire)* Please describe one possible source of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched.
5. *(Communicate)* Please write two paragraphs about what you did.


### Tutorial {.unnumbered}

Please follow the code of @hvitfeldt2021supervised in *Supervised Machine Learning for Text Analysis in R*, Chapter 5.2 "Understand word embeddings by finding them yourself", freely available [here](https://smltar.com/embeddings.html), to implement your own word embeddings for one year's worth of data from [Lipad](https://www.lipad.ca).
