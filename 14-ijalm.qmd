---
engine: knitr
---

# Linear models {#sec-its-just-a-linear-model}

**Required material**

- Read *Data Analysis Using Regression and Multilevel/Hierarchical Models*, Chapters 3 "Linear regression: the basics", 4 "Linear regression: before and after fitting the model", 5 "Logistic regression", and 6 "Generalized linear models", [@gelmanandhill]
- Read *An Introduction to Statistical Learning with Applications in R*, Chapters 3 "Linear Regression", and 4 "Classification", [@islr]
- Read *We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results*, [@cohn2016].
- Read *Why most published research findings are false*, [@ioannidis2005most]
- Read *Machine learning is going real-time*, [@chiphuyenone]
- Watch *Democratizing R with Plumber APIs*, [@democratizingr]
- Read *Science Storms the Cloud*, [@Gentemann2021]

**Key concepts and skills**

- Linear models are a key component of modelling, and enable us to consider a wide range of circumstance. Simple and multiple linear regression refer to the situation in which we explain one variable with one, and multiple, variable, respectively. Logistic and Poisson regression change the nature of the dependent variable, with it being a binary, and a count, respectively.
- We are concerned with two different aspects: prediction and inference. We use machine learning approaches for the former, and Bayesian methods for the latter. 
- Finally, putting a model into production requires a different set of skills to building it. For instance, a familiarity with a cloud provider and creating APIs. 

**Key packages and functions**

- Base R [@citeR]
  - `binomial()`
  - `glm()`
  - `lm()`
  - `rexp()`
  - `rnorm()`
  - `rpois()`
  - `sample()`
  - `set.seed()`
  - `summary()`
- `analogsea` [@citeanalogsea]
  - `debian_apt_get_install()`
  - `droplets()`
  - `install_github()`
  - `install_r_package()`
  - `key_create()`
- `beepr` [@beepr]
  - `beep()`
- `broom` [@broom]
  - `augment()`
  - `glance()`
  - `tidy()`
- `gutenbergr` [@gutenbergr]
	- `gutenberg_download()`
	- `gutenberg_works()`
- `modelsummary` [@citemodelsummary]
  - `modelsummary()`
- `plumber` [@plumber]
  - `do_deploy_api()`
  - `do_provision()`
  - `plumb()`
- `plumberDeploy` [@plumberdeploy]
- `poissonreg` [@poissonreg]
  - `poisson_reg()`
- `rstanarm` [@citerstanarm]
	- `default_prior_coef()`
	- `default_prior_intercept()`
	- `exponential()`
	- `gaussian()`
	- `posterior_vs_prior()`
	- `pp_check()`
	- `prior_summary()`
	- `stan_glm()`
- `ssh` [@ssh]
  - `ssh_key_info()`
- `tidymodels` [@citeTidymodels]
  - `parsnip` [@parsnip]
    - `fit()`
    - `linear_reg()`
    - `logistic_reg()`
    - `poisson_reg()`
    - `set_engine()`
  - `rsample` [@rsample]
    - `initial_split()`
    - `testing()`
    - `training()`
- `tidyverse` [@tidyverse]

## Introduction

Linear models have been around for a long time. For instance, speaking about the development of least squares, which is one way to fit linear models, in the 1700s, @stigler [p.16] describes how it was associated with foundational problems in astronomy, such as determining the motion of the moon and reconciling the non-periodic motion of Jupiter and Saturn. The fundamental issue at the time with least squares was that of hesitancy to combine different observations. Astronomers were early to develop a comfort with doing this because they had typically gathered their observations themselves and knew that the conditions of the data gathering were similar, even though the value of the observation was different. It took longer for social scientists to become comfortable with linear models, possibly because they were hesitant to group together data they worried was not alike. In this one sense, astronomers had an advantage because they could compare their predictions with what actually happened whereas this was more difficult for social scientists [@stigler, p. 163]. 

Francis Galton, mentioned in @sec-hunt-data, and others of his generation, some of whom were eugenicists, used linear regression in earnest in the late 1800s and early 1900s. Binary outcomes quickly became of interest and needed special treatment, leading to the development and wide adaption of logistic regression and similar methods in the mid-1900s [@cramer2002origins]. The generalized linear model framework came into being, in a formal sense, in the 1970s with @nelder1972generalized who brought this all together. Generalized linear models (GLMs) broaden the types of outcomes that are allowed. We still model outcomes as a linear function, but we are no longer constrained to the normal distribution. The outcome can be anything in the exponential family, and popular choices include the logistic distribution, and the Poisson distribution. A further generalization of GLMs is generalized additive models (GAMs) where we broaden the structure of the explanatory side. We still explain the dependent variable as an additive function of various bits and pieces, but those bits and pieces can be functions. This framework, in this way, came about in the 1990s, with @hastie1990generalized.

:::{.callout-note}
## Shoulders of giants

Dr Robert Tibshirani is Professor in the Departments of Statistics and Biomedical Data Science at Stanford University. After taking a PhD in Statistics from Stanford University in 1981, he joined the University of Toronto as an assistant professor. He was promoted to full professor in 1994, and moved to Stanford in 1998. He made fundamental contributions including GAMs and LASSO, which will be covered in @sec-text-as-data, and is an author of the influential @islr. He was awarded the COPSS Presidents' Award in 1996 and was appointed a Fellow of the Royal Society in 2019.
:::

When we build models, we are not discovering "the truth". A model is not, and cannot be, a true representation of reality. We are using the model to help us explore and understand the data that we have. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. When we use models, we are trying to understand the world, but there are enormous constraints on the perspective we bring to this. It is silly to expect one model to be universal. Further, we should not just blindly throw data into a regression model and hope that it will sort it out. "Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions" [@citemcelreath, p. 162].

We use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world; not the outcome. When we build models, we need to keep in mind both the world of the model and the broader world that we want to be able to speak about. To what extent does a model trained on the experiences of straight, cis, men, speak to the world as it is? It is not worthless, but it is also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world about which we would like to draw conclusions? We need to keep such questions front of mind.

Much of statistics was built up without concern for broader implications. And that was reasonable because it was developed for situations such as astronomy and agriculture. Folks were able to randomize the order of fields and planting because they worked at agricultural stations, for instance @fisherarrangement. But many of the subsequent applications in the twentieth and twenty-first centuries, do not have those properties. Statistics is often taught as though it proceeds through some idealized process where a hypothesis appears, is tested against some data that similarly appears, and is either confirmed or not. But that is not what happens. We react to incentives. We dabble, guess, and test, and then follow our intuition, backfilling as we need. All of this is fine. But it is not a world in which a traditional null hypothesis holds completely, which means concepts such as p-values and power lose some of their meaning. While we need to understand the "old world", we also need to be sophisticated enough to know when we need to move away from it. We can appreciate the beauty and ingenuity of a Ford Model T, at the same time recognizing we could not use it to win the Monaco Grand Prix.

Both manual checks and automated testing should be integrated into modelling. For instance, @wakefieldestimates built a model of excess deaths in a variety of countries, to estimate the overall death toll from the pandemic. After initially releasing the model, which had been extensively manually checked, some of the results were re-examined and it was found that the estimates for Germany and Sweden were over-sensitive. The authors acknowledged this, and addressed the issues, but the integration of automated testing, in addition to the usual manual checks, would go some way to enabling us to have more faith in the models of others.

In this chapter we begin with simple linear regression, and then move to multiple linear regression, the difference being the number of explanatory variables that we allow. We then consider logistic and Poisson regression. We go through three approaches for each of these: base R, which is useful when we want to quickly use the models in EDA; `tidymodels` [@citeTidymodels] which is useful when we are interested in prediction; and `rstanarm` [@citerstanarm] when we are interested in inference. In general, a model is either optimized for prediction or inference. Regardless of the approach we use, the important thing to remember is that modelling in this way is just fancy averaging.

For ease of next steps after this book, our notation for probability distributions follows @pitman, our notation for frequentist statistics follows @islr, and our notation for Bayesian statistics follows @citemcelreath.


## Simple linear regression

When we are interested in the relationship between two continuous variables, say $y$ and $x$, we can use simple linear regression. This is based on the Normal, also called "Gaussian", distribution. The shape of the Normal distribution is determined by two parameters, the mean, $\mu$, and the standard deviation, $\sigma$ [@pitman, p. 94]: 

$$y = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}z^2},$$
where $z = (x - \mu)/\sigma$ is the difference between the mean and $x$, in terms of the standard deviation.

As introduced in @sec-r-essentials, we use `rnorm()` to simulate data from the Normal distribution. 

```{r}
#| message: false
#| warning: false

library(tidyverse)

set.seed(853)

normal_example <-
  tibble(draws = rnorm(n = 20, mean = 0, sd = 1))

normal_example |> pull(draws)
```

Here we specified 20 draws from a Normal distribution with mean of 0 and standard deviation of 1. When we deal with real data, we will not know these parameters and we want to use our data to estimate them. We can estimate the mean, $\bar{x}$, and standard deviation, $\hat{\sigma}_x$, with the following estimators:

$$
\begin{aligned}
 \bar{x} &= \frac{1}{n} \times \sum_{i = 1}^{n}x_i\\
 \hat{\sigma}_{x} &= \sqrt{\frac{1}{n} \times \sum_{i = 1}^{n}\left(x_i - \bar{x}\right)^2}
\end{aligned}
$$

If $\hat{\sigma}_x$ is the estimate of the standard deviation, then a standard error of an estimate, say, $\bar{x}$ is:
$$\mbox{SE}(\bar{x})^2 = \frac{\sigma^2}{n}.$$

We can implement all these in code using our simulated data to see how close our estimates are.

```{r}
#| echo: false
#| eval: true
#| label: tbl-meanstdexample
#| tbl-cap: "Estimates of the mean and standard deviation based on the simulated data"

estimated_mean <-
  sum(normal_example$draws) / nrow(normal_example)

normal_example <-
  normal_example |>
  mutate(diff_square = (draws - estimated_mean) ^ 2)

estimated_standard_deviation <-
  sqrt(sum(normal_example$diff_square) / nrow(normal_example))

knitr::kable(
  tibble(mean = estimated_mean,
         sd = estimated_standard_deviation),
  col.names = c("Estimated mean",
                "Estimated std dev"),
  digits = 2
)
```

We should not be worried that our estimates are slightly off (@meanstdexample). We only considered 20 observations. It will typically take a larger number of draws before we get the expected shape, and our estimated parameters get close to the actual parameters, but it will happen (@fig-normaldistributiontakingshape). @wasserman [p. 76] describes our certainty of this, which is due to the Law of Large Numbers, as "a crowning achievement in probability".

```{r}
#| eval: true
#| fig-cap: "The Normal distribution takes its familiar shape as the number of draws increases"
#| include: true
#| label: fig-normaldistributiontakingshape
#| message: false
#| warning: false

library(tidyverse)

set.seed(853)

normal_takes_shape <-
  tibble(
    number_of_draws = c(),
    draws = c()
  )

for (i in c(2, 5, 10, 50, 100, 500, 1000, 10000, 100000)) {
  draws_i <-
    tibble(
      number_of_draws = c(rep.int(
        x = paste(as.integer(i), " draws"),
        times = i
      )),
      draws = c(rnorm(
        n = i,
        mean = 0,
        sd = 1
      ))
    )

  normal_takes_shape <- rbind(normal_takes_shape, draws_i)
  rm(draws_i)
}

normal_takes_shape |>
  mutate(number_of_draws = as_factor(number_of_draws)) |>
  ggplot(aes(x = draws)) +
  geom_density() +
  theme_minimal() +
  facet_wrap(
    vars(number_of_draws),
    scales = "free_y"
  ) +
  labs(
    x = "Draw",
    y = "Density"
  )
```

When we use simple linear regression, we assume that our relationship is characterized by the variables and the parameters. If we have two variables, $Y$ and $X$, then we could characterize a linear relationship between these as [@islr, p. 61]:
$$
Y \approx \beta_0 + \beta_1 X.
$$ {#eq-xandy}

Here, there are two coefficients, also referred to as "parameters": the "intercept", $\beta_0$, and the "slope", $\beta_1$. In @eq-xandy we are saying that $Y$ will have some value, $\beta_0$, even when $X$ is 0, and that $Y$ will change by $\beta_1$ units for every one unit change in $X$. We may then take this relationship to the particular data that we have, in order to estimate these coefficients.

To make this example concrete, we will simulate some data and then discuss it in that context. For instance, we could consider the time it takes someone to run five kilometers, compared with the time it takes them to run a marathon (@fig-fivekmvsmarathon-1). We specify a relationship of 8.4, as that is roughly the ratio between the distance of a marathon and a five-kilometer run. Notice that it is the noise that is normally distributed, not the variables.

```{r}
#| eval: true
#| include: true
#| label: fig-simulatemarathondata
#| message: false
#| warning: false

set.seed(853)

number_of_observations <- 100
expected_relationship <- 8.4
very_fast_5km_time <- 15
good_enough_5km_time <- 30

sim_run_data <-
  tibble(
    five_km_time =
      runif(n = number_of_observations,
            min = very_fast_5km_time,
            max = good_enough_5km_time),
    noise = rnorm(n = number_of_observations, mean = 0, sd = 20),
    marathon_time = five_km_time * expected_relationship + noise
  ) |>
  mutate(
    five_km_time = round(x = five_km_time, digits = 1),
    marathon_time = round(x = marathon_time, digits = 1)
  ) |>
  select(-noise)

sim_run_data
```


```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-fivekmvsmarathon
#| fig-cap: "Simulated data of the relationship between the time to run five kilometers and a marathon"
#| fig-subcap: ["Distribution of simulated data", "With one 'linear best-fit' line illustrating the implied relationship", "Including standard errors"]
#| layout-ncol: 2

base_sim_run_data <-
  sim_run_data |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point() +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)") +
  theme_classic()

base_and_fit_sim_run_data <-
  base_sim_run_data +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "black",
    linetype = "dashed",
    formula = "y ~ x"
  )

base_and_fit_and_se_sim_run_data <-
  base_sim_run_data +
  geom_smooth(
    method = "lm",
    se = TRUE,
    color = "black",
    linetype = "dashed",
    formula = "y ~ x"
  )

base_sim_run_data

base_and_fit_sim_run_data

base_and_fit_and_se_sim_run_data
```

In this simulated example, we know the true values of $\beta_0$ and $\beta_1$. But our challenge is to see if we can use only the data, and simple linear regression, to recover them. That is, can we use $x$, which is the five-kilometer time, to produce estimates of $y$, which is the marathon time (by convention, hats are used to indicate that these are, or will be, estimated values) [@islr, p. 61]:

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.$$

This involves estimating values for $\beta_0$ and $\beta_1$. But how should we estimate these coefficients? Even if we impose a linear relationship there are many options, because a large number of straight lines could be drawn. But some of those lines would fit the data better than others.

One way we may define a line as being "better" than another, is if it is as close as possible to each of the $x$ and $y$ combinations that we know. There are a lot of candidates for how we define "as close as possible", but one is to minimize the residual sum of squares. To do this we produce estimates for $\hat{y}$ based on some guesses of $\hat{\beta}_0$ and $\hat{\beta}_1$, given the $x$. We then work out how "wrong", for every observation $i$, we were [@islr, p. 62]:
$$e_i = y_i - \hat{y}_i.$$

To compute the residual sum of squares (RSS), we sum across all the points, taking the square to account for negative differences [@islr, p. 62]:
$$\mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.$$
This results in one "linear best-fit" line (@fig-fivekmvsmarathon-2), but it is worth reflecting on all of the assumptions and decisions that it took to get us to this point.

Underpinning our use of simple linear regression is a belief that there is some "true" relationship between $X$ and $Y$, that is [@islr, p. 62]: 

$$Y = f(X) + \epsilon.$$

We are going to say that function, $f()$, is linear, and so for simple linear regression [@islr, p. 62]:

$$\hat{Y} = \beta_0 + \beta_1 X + \epsilon.$$

We do not, and cannot, know the "true" relationship between $X$ and $Y$. All we can do is use our sample to estimate it. But because our understanding depends on that sample, for every possible sample; we would get a slightly different relationship, as measured by the coefficients. 

That $\epsilon$ is a measure of our error---what does the model not know in the small, contained, world implied by this dataset? There is going to be plenty that the model does not know, but we hope the error does not depend on $X$, and that the error is normally distributed.

We can conduct simple linear regression with `lm()` from base R. We specify the dependent variable first, then `~`, followed by the independent variables. Finally, we specify the dataset. Before we run a regression we may want to include a quick check of the class of the variables, and the number of observations, just to ensure that it corresponds with what we were expecting. An we may check that our estimate seems reasonable, for instance, based on our knowledge of the respective distances of a five-kilometer run and a marathon, than $beta_1$ is somewhere between 6 and 10.

```{r}
stopifnot(
  class(sim_run_data$marathon_time) == "numeric",
  class(sim_run_data$five_km_time) == "numeric",
  nrow(sim_run_data) == 100
)

sim_run_data_first_model <-
  lm(
    marathon_time ~ five_km_time,
    data = sim_run_data
  )

stopifnot(between(sim_run_data_first_model$coefficients[2],
                  6, 10))
```

To see the result of the regression, we can use `modelsummary()` from `modelsummary` [@citemodelsummary] (@tbl-modelsummaryfivekmonly).

```{r}
#| label: tbl-modelsummaryfivekmonly
#| tbl-cap: "Explaining marathon times based on five-kilometer run times"

modelsummary::modelsummary(
  list(
    "Five km only" = sim_run_data_first_model
  ),
  fmt = 2
)
```

The top half of the table provides our estimated coefficients and standard errors. And the second half provides some useful diagnostics. 
The intercept is the marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if the five-kilometer run time changed by one unit. In this case it is about eight, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.

We use `augment()` from `broom` [@broom] to add the fitted values and residuals to our original dataset. This allows us to plot the residuals (@fig-fivekmvsmarathonresids).

```{r}
sim_run_data <-
  broom::augment(
    sim_run_data_first_model,
    data = sim_run_data
  )

sim_run_data
```

```{r}
#| eval: true
#| include: true
#| message: false
#| warning: false
#| label: fig-fivekmvsmarathonresids
#| layout-ncol: 2
#| fig-cap: "Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon"
#| fig-subcap: ["Distribution of residuals","Residuals by order", "Comparing the estimated time with the actual time"]

ggplot(
  sim_run_data,
  aes(x = .resid)
) +
  geom_histogram(binwidth = 1) +
  theme_classic() +
  labs(
    y = "Number of occurrences",
    x = "Residuals"
  )

ggplot(
  sim_run_data,
  aes(x = five_km_time, y = .resid)
) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dotted", color = "grey") +
  theme_classic() +
  labs(
    y = "Residuals",
    x = "Five-kilometer time (minutes)"
  )

ggplot(
  sim_run_data,
  aes(x = marathon_time, .fitted)
) +
  geom_point() +
  theme_classic() +
  labs(
    y = "Estimated marathon time",
    x = "Actual marathon time"
  )
```

We want our estimate to be unbiased. When we say our estimate is unbiased, we are trying to say that even though for some sample our estimate might be too high, and for another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. That is, an estimator is unbiased if it does not systematically over- or under-estimate [@islr, p. 65].

But we want to try to speak to the "true" relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyze. And this is where standard error comes in. It tells us how off our estimates are compared with the actual data (@fig-fivekmvsmarathon-3).

From standard errors, we can compute a confidence interval. A 95 per cent confidence interval is a range, such that there is a 0.95 probability that the interval contains the population parameter, which is typically unknown [@islr, p. 66]. The lower end of this range is: $\hat{\beta_1} - 2 \times \mbox{SE}\left(\hat{\beta_1}\right)$ and the upper end of this range is: $\hat{\beta_1} + 2 \times \mbox{SE}\left(\hat{\beta_1}\right)$.

Now that we have a range, for which we can say there is a 95 per cent probability that the range contains the true population parameter, we could test claims. For instance, we could claim that there is no relationship between $X$ and $Y$, i.e. $\beta_1 = 0$, as an alternative to a claim that there is some relationship between $X$ and $Y$, i.e. $\beta_1 \neq 0$.

In the same way that in @sec-hunt-data we needed to decide how much evidence it would take to convince us that our tea taster could distinguish whether milk or tea had been added first, we need to decide whether our estimate of $\beta_1$, which is $\hat{\beta}_1$, is "far enough" away from zero for us to be comfortable claiming that $\beta_1 \neq 0$. How far is "far enough"? If we were very confident in our estimate of $\beta_1$ then it would not have to be far, but if we were not, then it would have to be substantial. The standard error of $\hat{\beta}_1$ does an awful lot of work here in accounting for a variety of factors, only some of which it can actually account for. 

We compare this standard error with $\hat{\beta}_1$ to get the t-statistic:
$$t = \frac{\hat{\beta}_1 - 0}{\mbox{SE}(\hat{\beta}_1)}.$$ 
And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if it was actually the case that $\beta_1 = 0$. This probability is the p-value. A smaller p-value means it is less likely that we would observe our data due to chance if there was not an actual relationship.

> Words! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?
>
> *The Picture of Dorian Gray* [@wilde].

We will not make much use of p-values in this book because they are a specific and subtle concept. They are difficult to understand and easy to abuse. Even though they are "little help" for "scientific inference" many disciplines are incorrectly fixated on them [@nelderdoesntmiss, p. 257]. One issue is that they embody, and assume correct, every assumption of the model, including everything that went into gathering and cleaning the data. While smaller p-values do imply the data are more unusual if all the assumptions were correct; when we consider the full data science workflow there are usually an awful lot of assumptions. And we do not get guidance from p-values about whether the assumptions are reasonable [@greenland2016statistical p. 339]. 

A p-value may reject a null hypothesis because the null hypothesis is actually false, but it may also be that some data were incorrectly gathered or prepared. We can only be sure that the p-value speaks to the hypothesis we are interested in testing, if all the other assumptions are correct. There is nothing inherently wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways. @coxtalks provides a lovely discussion of what this requires.

One application where it is easy to see abuse of p-values is in power analysis. Power, in a statistical sense, refers to probability of rejecting a null hypothesis that is actually false. As power relates to hypothesis testing, it also related to sample size. There is often a worry that a study is "under-powered", meaning there was not a large enough sample, but rarely a worry that, say, the data were inappropriately cleaned, even though we cannot distinguish between these based only on a p-value.

:::{.callout-note}
## Shoulders of giants

Dr Nancy Reid is University Professor in the Department of Statistical Sciences at the University of Toronto. After obtaining a PhD in Statistics from Stanford University in 1979, she took a position as a postdoctoral fellow at Imperial College London. She was then appointed as assistant professor at the University of British Columbia in 1980, and then moved to the University of Toronto in 1986, where she was promoted to full professor in 1988 and served as department chair between 1997 and 2002 [@Staicu2017]. Her research focuses on obtaining accurate inference in small-sample regimes and developing inferential procedures for complex models featuring intractable likelihoods. @cox1987parameter examines how re-parameterizing models can simplify inference, @varin2011overview surveys methods for approximating intractable likelihoods, and @reid2003asymptotics overviews inferential procedures in the small-sample regime. Dr Reid was awarded the 1992 COPSS Presidentsâ€™ Award, the 2016 Guy Medal in Silver, and the 2022 Guy Medal in Gold from the Royal Statistical Society.
:::

## Multiple linear regression

To this point we have just considered one explanatory variable. But we will usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different.

We may also like to consider explanatory variables that do not have an inherent ordering. For instance: pregnant or not; day or night. When there are only two options then we can use a binary variable, which is considered either 0 or 1. If we have a column of character values that only has two values, such as: `c("Myles", "Ruth", "Ruth", "Myles", "Myles", "Ruth")`, then using this as an explanatory variable in the usual regression set up, would mean that it is treated as a binary variable. If there are more than two levels then we can use a combination of binary variables, where the "missing" outcome (baseline) gets pushed into the intercept. 

As an example, we add whether it was raining to our relationship between marathon and five-kilometer run times. We then specify that if it was raining then the person is five minutes slower than they otherwise would be.

```{r}
minutes_slower_in_rain <- 5

sim_run_data <-
  sim_run_data |>
  mutate(was_raining = sample(
    c("Yes", "No"),
    size = number_of_observations,
    replace = TRUE,
    prob = c(0.2, 0.8)
  )) |>
  mutate(
    marathon_time = if_else(
      was_raining == "Yes",
      marathon_time + minutes_slower_in_rain,
      marathon_time
    )
  ) |>
  select(five_km_time, marathon_time, was_raining)

sim_run_data
```

We can add additional explanatory variables to `lm()` with `+` (@tbl-modelsummaryruntimes). Again, we will include a variety of quick tests for class and the number of observations, and also add another about missing values. We may not have any idea what the coefficient on the affect of rain should be, but if we did not expect it to make them faster then we could also add a test of that with a wide interval.

```{r}
stopifnot(
  class(sim_run_data$marathon_time) == "numeric",
  class(sim_run_data$five_km_time) == "numeric",
  class(sim_run_data$was_raining) == "character",
  all(complete.cases(sim_run_data)),
  nrow(sim_run_data) == 100
)

sim_run_data_rain_model <-
  lm(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data
  )

stopifnot(between(sim_run_data_rain_model$coefficients[3],
                  0, 20))
```

The result corresponds with what we expect if we look at a plot of the data (@fig-fivekmvsmarathonbinary).

```{r}
#| eval: true
#| fig-cap: "Simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon, with a binary variable for whether it was raining"
#| include: true
#| label: fig-fivekmvsmarathonbinary
#| message: false
#| warning: false

sim_run_data |>
  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +
  geom_point() +
  geom_smooth(
    method = "lm",
    color = "black",
    linetype = "dashed",
    formula = "y ~ x"
  ) +
  labs(
    x = "Five-kilometer time (minutes)",
    y = "Marathon time (minutes)",
    color = "Was raining"
  ) +
  theme_classic() +
  scale_color_brewer(palette = "Set1")
```

In addition to wanting to include additional explanatory variables, we may think that they are related to each another. For instance, maybe rain really matters if it is also very humid that day. So we are then interested in the humidity and the temperature, but also how those two variables interact. We can do this by using `*` instead of `+` when we specify the model. When we interact variables in this way, then we almost always need to include the individual variables as well and `lm()` will do this by default (@tbl-modelsummaryruntimes).

```{r}
minutes_slower_in_high_humidity <- 2

sim_run_data <-
  sim_run_data |>
  mutate(humidity = sample(
    c("High", "Low"),
    size = number_of_observations,
    replace = TRUE,
    prob = c(0.2, 0.8)
  )) |>
  mutate(
    marathon_time = if_else(
      humidity == "High",
      marathon_time + minutes_slower_in_high_humidity,
      marathon_time
    )
  )

sim_run_data
```

```{r}
#| include: true
#| label: tbl-modelsummaryruntimes
#| tbl-cap: "Explaining marathon times based on five-kilometer run times and weather features"

stopifnot(
  class(sim_run_data$marathon_time) == "numeric",
  class(sim_run_data$five_km_time) == "numeric",
  class(sim_run_data$was_raining) == "character",
  class(sim_run_data$humidity) == "character",
  all(complete.cases(sim_run_data)),
  nrow(sim_run_data) == 100,
  unique(sim_run_data$humidity) %in% c("Low", "High")
)

sim_run_data_rain_and_humidity_model <-
  lm(
    marathon_time ~ five_km_time + was_raining * humidity,
    data = sim_run_data
  )

modelsummary::modelsummary(
  list(
    "Five km only" = sim_run_data_first_model,
    "Add rain" = sim_run_data_rain_model,
    "Add humidity" = sim_run_data_rain_and_humidity_model
  ),
  fmt = 2
)
```

There are a variety of threats to the validity of linear regression estimates, and aspects to think about. We need to address these when we use it, and usually graphs and associated text are sufficient to assuage most of these. Aspects of concern include:

1. Linearity of explanatory variables. We are concerned with whether the independent variables enter in a linear way. Sometimes if we are worried that there might be a multiplicative relationship between the explanatory variables, rather than an additive one, then we may consider a logarithmic transform. We can usually be convinced there is enough linearity in our explanatory variables for our purposes by using graphs of the variables.
2. Independence of errors. We are concerned that the errors are not correlated. For instance, if we are interested in weather-related measurement such as average daily temperature, then we may find a pattern because the temperature on one day is likely similar to the temperature on another. We can be convinced that we have satisfied this condition by making graphs of the errors, such as @fig-fivekmvsmarathonresids.
3. Homoscedasticity of errors. We are concerned that the errors are not becoming systematically larger or smaller throughout the sample. If that is happening, then we term it heteroscedasticity. Again, graphs of errors, such as @fig-fivekmvsmarathonresids are used to convince us of this.
4. Normality of errors. We are concerned that our errors are normally distributed when we are interested in making individual-level predictions.
5. Outliers and other high-impact observations. Finally, we might be worried that our results are being driven by a handful of observations. For instance, thinking back to @sec-static-communication and Anscombe's Quartet, we notice that linear regression estimates would be heavily influenced by the inclusion of one or two particular points. We can become comfortable with this by considering our analysis on various sub-sets

Those aspects are statistical concerns and relate to whether the model is working. The most important threat to validity and hence the aspect that must be addressed at some length, is speaking to the fact that this model is appropriate to the circumstances and addresses the research question at hand.

## Two cultures

@breiman2001statistical describes two cultures: one focused on prediction and the other on inference. More recently, things are a little less distinguished.

We will use `tidymodels` when we are interested in prediction. But when we are focused on inference, instead, we will use Bayesian approaches. To do this we use the probabilistic programming language "Stan", and interface with it using `rstanarm` [@citerstanarm]. This is slightly simplistic---we could do prediction. with Bayesian models, and we could use Stan approaches within `tidymodels`. But, for the sake of simplicity and clarity, we keep these separated in this way for this book. It is not a bad idea to be conversant in both ecosystems in any case.

### Prediction

When we are focused on prediction, we will often want to fit many models. One way to do this is to copy and paste code many times. There is nothing wrong with that. And that is the way that most people get started. But we need an approach that: 

1) scales more easily; 
2) enables us to think carefully about over-fitting; and 
3) adds model evaluation.

The use of `tidymodels` [@citeTidymodels] satisfies these criteria by providing a coherent grammar that allows us to easily fit a variety of models. Like `tidyverse`, it is a package of packages.

As we are focused on prediction, we are worried about over-fitting our data, which would limit our ability to make claims about other datasets. One way to partially address this is to split our dataset into training and test datasets using `initial_split()`. 

```{r}
#| message: false
#| warning: false

library(tidymodels)

set.seed(853)

sim_run_data_split <-
  initial_split(
    data = sim_run_data,
    prop = 0.80
  )

sim_run_data_split
```

Having split the data, we then create the training and test datasets with `training()` and `testing()`.

```{r}
sim_run_data_train <- training(sim_run_data_split)

sim_run_data_test <- testing(sim_run_data_split)
```

When we look at the training and test datasets, we can see that we have placed most of our dataset into the training dataset. We will use that to estimate the parameters of our model (@tbl-modelsummarybayes). We have kept a small amount of it back, and we will use that to evaluate our model.

```{r}
sim_run_data_first_model_tidymodels <-
  linear_reg() |>
  set_engine(engine = "lm") |>
  fit(
    marathon_time ~ five_km_time + was_raining,
    data = sim_run_data_train
  )
```

Why might we do this? Our concern is the bias-variance trade-off, which haunts all aspects of modelling. In particular, we are concerned that our results may be too particular to the dataset that we have, such that they are not applicable to other datasets. To take an extreme example, consider a dataset with ten observations. We could come up with a model that perfectly hits those observations. But when we took that model to other datasets, even those generated by the same underlying process, it would not be accurate.

One way to deal with this concern is to split the data in this way. We use the training data to inform our estimates of the coefficients, and then using the test data, to evaluate the model. A model that too closely matched the data in the training data would not do well in the test data, because it would be too specific to the training data. The use of this test-training split enables us the opportunity to build an appropriate model.

It is more difficult to do this separation appropriately than one might initially think. We want to avoid the situation where aspects of the test dataset are present in the training dataset because this inappropriately telegraphs what is about to happen. But if we consider data cleaning and preparation, which likely involves the entire dataset, it may be that some features of each are influencing each other. @kapoornarayanan2022 find extensive data leakage in applications of machine learning that could invalidate much research.

:::{.callout-note}
## Shoulders of giants

Dr Daniela Witten is the Dorothy Gilford Endowed Chair of Mathematical Statistics and Professor of Statistics & Biostatistics at the University of Washington. After taking a PhD in Statistics from Stanford University in 2010, she joined the University of Washington as an assistant professor. She was promoted to full professor in 2018. One active area of her research is double-dipping which is focused on the effect of sample splitting [@selectiveinference]. She is an author of the influential @islr. She was appointed a Fellow of the American Statistical Association in 2020 and awarded the COPSS Presidents' Award in 2022.
:::

We will focus on `tidymodels` and prediction-focused approaches in @sec-text-as-data when we consider text as data.

### Inference

In order to use Bayesian approaches we will need to specify a starting point. This is a guess for what we think the coefficients will be, and is called a "prior". This is another reason for the workflow advocated in this book; the simulate stage leads directly to priors. We will also more thoroughly specify the model that we are interested in.

$$
\begin{aligned}
y_i &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i\\
\beta_0 &\sim \mbox{Normal}(0, 3) \\
\beta_1 &\sim \mbox{Normal}(0, 3) \\
\sigma &\sim \mbox{Exponential}(1) \\
\end{aligned}
$$

On a practical note, one aspect that is different between Bayesian approaches and the way we have been doing modelling to this point, is that Bayesian models will usually take longer to run. Because of this, it can be useful to run the model, either within the Quarto document or in a separate R script, and then save it with `saveRDS()`. With sensible Quarto chunk options (see @sec-reproducible-workflows), the model can then be read into the Quarto document with `readRDS()`. In this way, the model delay is only imposed once for a given model (@tbl-modelsummarybayes). (It can be useful to add `beep()` from `beepr` [@beepr] to the end of the model, to get an audio notification when the model is done, and there are a variety of fun sounds.) 

```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false

library(rstanarm)

sim_run_data_first_model_rstanarm <-
  stan_glm(
    formula = marathon_time ~ five_km_time + was_raining,
    data = sim_run_data,
    family = gaussian(),
    prior = normal(0, 3),
    prior_intercept = normal(0, 3),
    prior_aux = exponential(rate = 1),
    seed = 853
  )

beepr::beep()

saveRDS(
  sim_run_data_first_model_rstanarm,
  file = "sim_run_data_first_model_rstanarm.rds"
)
```

```{r}
#| echo: false
#| eval: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  sim_run_data_first_model_rstanarm,
  file = "outputs/model/sim_run_data_first_model_rstanarm.rds"
)
```

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

sim_run_data_first_model_rstanarm <-
  readRDS(file = "sim_run_data_first_model_rstanarm.rds")
```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

sim_run_data_first_model_rstanarm <-
  readRDS(file = "outputs/model/sim_run_data_first_model_rstanarm.rds")
```

```{r}
#| label: tbl-modelsummarybayes
#| tbl-cap: "Forecasting and explanatory models of marathon times based on five-kilometer run times"
#| warning: false

library(modelsummary)

modelsummary(
  list(
    "tidymodels" = sim_run_data_first_model_tidymodels,
    "rstanarm" = sim_run_data_first_model_rstanarm
  ),
  statistic = "conf.int"
)
```

The issue of picking priors is a challenging one and the subject of extensive study. For the purposes of this book, just using the `rstanarm` defaults is fine. But even if they are just the default, priors should be explicitly specified in the model and included in the function. This is to make it clear to others what has been done, and ensure that even if the `rstanarm` defaults change the model will not. We can use `default_prior_intercept()` and `default_prior_coef()` to find the default priors in `rstanarm` and then explicitly include them in the model. It is normal to find it difficult to know what prior to specify when getting started. Getting started by adapting someone else's code is perfectly fine. If they have not specified their priors, then we can use `prior_summary()`, to find out which priors were used. And `rstanarm` has a helper available to us that we should use. Specify the prior that you think reasonable, include it in the model, but also include "autoscale = TRUE", and `rstanarm` will adjust the scale.

```{r}
prior_summary(sim_run_data_first_model_rstanarm)
```

We will not make much of it in this book, but we will be interested in understanding what the priors imply before we involve any data. We do this by implementing prior predictive checks. In particular, simulating from the priors to look at what they imply about our coefficients.

```{r}
number_of_draws <- 100

priors <-
  tibble(
    sigma = rep(rexp(n = number_of_draws, rate = 1), times = 16),
    beta_0 = rep(rnorm(n = number_of_draws, mean = 0, sd = 3), times = 16),
    beta_1 = rep(rnorm(n = number_of_draws, mean = 0, sd = 3), times = 16),
    five_km_time = rep(15:30, each = number_of_draws),
    mu = beta_0 + beta_1 * five_km_time
  ) |>
  rowwise() |>
  mutate(
    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)
  )
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-worstmodelever
#| fig-cap: "Some implications from the priors that were used"
#| layout-ncol: 2
#| fig-subcap: ["Distribution of implied marathon times", "Relationship between 5km and marathon times"]

priors |>
  ggplot(aes(x = marathon_time)) +
  geom_freqpoly(binwidth = 3)

priors |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point()
```

It is clear from @fig-worstmodelever that our model has been poorly constructed. In particular, not only are there world record marathon times, there are negative marathon times! In particular, our prior for $\beta_1$ does not take in all the information that we know. We know that a marathon is about eight times longer than a 5km run and so we should center it around that. To account for that information, we could change the centering of the prior on our $\beta_1$ coefficient. Our re-specified model is:

$$
\begin{aligned}
y_i &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i\\
\beta_0 &\sim \mbox{Normal}(0, 3) \\
\beta_1 &\sim \mbox{Normal}(8, 3) \\
\sigma &\sim \mbox{Exponential}(1) \\
\end{aligned}
$$
And we can see from prior prediction checks that it seems more reasonable (@fig-secondworstmodelever).

```{r}
number_of_draws <- 100

updated_priors <-
  tibble(
    sigma = rep(rexp(n = number_of_draws, rate = 1), times = 16),
    beta_0 = rep(rnorm(n = number_of_draws, mean = 0, sd = 3), times = 16),
    beta_1 = rep(rnorm(n = number_of_draws, mean = 8, sd = 3), times = 16),
    five_km_time = rep(15:30, each = number_of_draws),
    mu = beta_0 + beta_1 * five_km_time
  ) |>
  rowwise() |>
  mutate(
    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)
  )
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| fig-cap: "Updated priors"
#| label: fig-secondworstmodelever
#| layout-ncol: 2
#| fig-subcap: ["Distribution of implied marathon times","Relationship between 5km and marathon times"]

updated_priors |>
  ggplot(aes(x = marathon_time)) +
  geom_freqpoly(binwidth = 3)

updated_priors |>
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point()
```

Then we can re-run our model with these updated priors (@tbl-modelsummarybayesbetter).

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

sim_run_data_second_model_rstanarm <-
  stan_glm(
    formula = marathon_time ~ five_km_time + was_raining,
    data = sim_run_data,
    family = gaussian(),
    prior = normal(location = 8, scale = 3, autoscale = TRUE),
    prior_intercept = normal(0, 1, autoscale = TRUE),
    prior_aux = exponential(rate = 1, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  sim_run_data_first_model_rstanarm,
  file = "sim_run_data_second_model_rstanarm.rds"
)
```

```{r}
#| include: false
#| message: false
#| warning: false
#| eval: false

# INTERNAL
saveRDS(
  sim_run_data_second_model_rstanarm,
  file = "outputs/model/sim_run_data_second_model_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| warning: false
#| message: false

library(rstanarm)

sim_run_data_second_model_rstanarm <-
  readRDS(file = "outputs/model/sim_run_data_second_model_rstanarm.rds")
```

```{r}
#| label: tbl-modelsummarybayesbetter
#| tbl-cap: "Forecasting and explanatory models of marathon times based on five-kilometer run times"
#| warning: false

modelsummary(
  list(
    "tidymodels" = sim_run_data_first_model_tidymodels,
    "rstanarm" = sim_run_data_first_model_rstanarm,
    "rstanarm better priors" = sim_run_data_second_model_rstanarm
  ),
  statistic = "conf.int"
)
```

We used the "auto = TRUE" option, so it can be helpful to look at how they were updated with `prior_summary()`.

```{r}
prior_summary(sim_run_data_second_model_rstanarm)
```

Having built a Bayesian model, we may want to look at what the posterior predicts (@fig-ppcheckandposteriorvsprior). The idea is that if the model is doing a good job of fitting the data then it the posterior should be able to be used to simulate data that are similar to the actual data [@bayesianworkflow]. We can implement a posterior predictive check with `pp_check()`. And we can compare the posterior with the prior with `posterior_vs_prior()`. Helpfully, `pp_check()` and `posterior_vs_prior()` return `ggplot` objects so we can modify the look of them in the normal way we manipulate graphs.

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(sim_run_data_second_model_rstanarm) +
  theme_classic()

posterior_vs_prior(sim_run_data_second_model_rstanarm) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```




## Logistic regression

Linear regression is a useful way to come to better understand our data. But it assumes a continuous outcome variable which can take any number on the real line. We would like some way to use this same machinery when we cannot satisfy this condition. We turn to logistic and Poisson regression for binary and count outcome variables, respectively, noting they are still linear models, because the independent variables enter in a linear fashion.

Logistic regression, and its close variants, are useful in a variety of settings, from elections [@wang2015forecasting] through to horse racing [@chellel2018gambler; @boltonruth]. We use logistic regression when the dependent variable is a binary outcome, such as 0 or 1. Although the presence of a binary outcome variable may sound limiting, there are a lot of circumstances in which the outcome either naturally falls into this situation, or can be adjusted into it.

One reason to use logistic regression is that we will be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. The foundation of logistic regression is the logit function:

$$
\mbox{logit}(x) = \log\left(\frac{x}{1-x}\right).
$$
This will transpose values between 0 and 1, onto the real line. For instance, `logit(0.1) = -2.2`, `logit(0.5) = 0`, and `logit(0.9) = 2.2`.

We will simulate data on whether it is day or night, based on the number of cars that we can see.

```{r}
#| message: false
#| warning: false

library(tidyverse)

set.seed(853)

day_or_night <-
  tibble(
    number_of_cars = runif(n = 1000, min = 0, 100),
    noise = rnorm(n = 1000, mean = 0, sd = 10),
    is_night = if_else(number_of_cars + noise > 50, 1, 0)
  ) |>
  mutate(number_of_cars = round(number_of_cars)) |>
  select(-noise)

day_or_night
```

As with linear regression, when it comes to logistic regression we can use `glm()` from base to put together a quick model and `summary()` to look at it. In this case we will try to work out whether it is day or night, based on the number of cars we can see. We are interested in estimating @eq-logisticexample:
$$
\mbox{Pr}(y_i=1) = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right).
$$ {#eq-logisticexample}

```{r}
stopifnot(
  class(day_or_night$is_night) == "numeric",
  class(day_or_night$number_of_cars) == "numeric",
  nrow(day_or_night) == 1000
)

day_or_night_model <-
  glm(
    is_night ~ number_of_cars,
    data = day_or_night,
    family = "binomial"
  )
```

One reason that logistic regression can be a bit tricky initially, is because the coefficients take a bit of work to interpret. In particular, our estimate of the likelihood of it being night is 0.91 This is the odds. So, the odds that it is night, increase by 0.91 as the number of cars that we saw increases. We can translate the result into probabilities using `augment()` from `broom` [@broom] and this allows us to graph the results (@fig-dayornightprobs).

```{r}
day_or_night <-
  broom::augment(
    day_or_night_model,
    data = day_or_night,
    type.predict = "response"
  )

day_or_night
```

```{r}
#| eval: true
#| fig-cap: "Logistic regression probability results with simulated data of whether it is day or night based on the number of cars that are around"
#| include: true
#| label: fig-dayornightprobs
#| message: false
#| warning: false

day_or_night |>
  mutate(is_night = factor(is_night)) |>
  ggplot(aes(
    x = number_of_cars,
    y = .fitted,
    color = is_night
  )) +
  geom_jitter(width = 0.01, height = 0.01, alpha = 0.3) +
  labs(
    x = "Number of cars that were seen",
    y = "Estimated probability it is night",
    color = "Was actually night"
  ) +
  theme_classic() +
  scale_color_brewer(palette = "Set1")
```

We can use `tidymodels` to run this if we wanted. In order to do that, we first need to change the class of our dependent variable into a factor.

```{r}
set.seed(853)

day_or_night <-
  day_or_night |>
  mutate(is_night = as_factor(is_night))

day_or_night_split <- initial_split(day_or_night, prop = 0.80)
day_or_night_train <- training(day_or_night_split)
day_or_night_test <- testing(day_or_night_split)

stopifnot(
  class(day_or_night_train$is_night) == "factor",
  class(day_or_night_train$number_of_cars) == "numeric",
  nrow(day_or_night_train) == 800
)

day_or_night_tidymodels <-
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(
    is_night ~ number_of_cars,
    data = day_or_night_train
  )
```

As before, we can make a graph of the actual results compared with our estimates. But one nice aspect of this is that we could use our test dataset to more thoroughly  evaluate our model's forecasting ability, for instance through a confusion matrix. We find that the model does well on the held-out dataset.

```{r}
day_or_night_tidymodels |>
  predict(new_data = day_or_night_test) |>
  cbind(day_or_night_test) |>
  conf_mat(truth = is_night, estimate = .pred_class)
```

We might be interested in inference, and so want to build a Bayesian model using `rstanarm`. Again, we will more fully specify our model: 

$$
\begin{aligned}
\mbox{Pr}(y_i=1) & = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right)\\
\beta_0 & \sim \mbox{Normal}(0, 3)\\
\beta_1 & \sim \mbox{Normal}(0, 3)
\end{aligned}
$$

```{r}
#| eval: false
#| echo: true
#| message: false
#| warning: false

stopifnot(
  class(day_or_night$is_night) == "factor",
  class(day_or_night$number_of_cars) == "numeric",
  nrow(day_or_night) == 1000
)

day_or_night_rstanarm <-
  stan_glm(
    is_night ~ number_of_cars,
    data = day_or_night,
    family = binomial(link = "logit"),
    prior = normal(location = 0, scale = 3, autoscale = TRUE),
    prior_intercept = normal(location = 0, scale = 3, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  day_or_night_rstanarm,
  file = "day_or_night_rstanarm.rds"
)
```

```{r}
#| eval: false
#| include: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  day_or_night_rstanarm,
  file = "outputs/model/day_or_night_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

day_or_night_rstanarm <-
  readRDS(file = "outputs/model/day_or_night_rstanarm.rds")
```

We can compare our different models (@tbl-modelsummarylogistic).

```{r}
#| label: tbl-modelsummarylogistic
#| tbl-cap: "Forecasting and explanatory models of whether it is day or night, based on the number of cars on the road"

modelsummary(
  list(
    "base R" = day_or_night_model,
    "tidymodels" = day_or_night_tidymodels,
    "rstanarm" = day_or_night_rstanarm
  ),
  statistic = "conf.int"
)
```

## Poisson regression

When we have count data of binary outcomes, we should initially think to use Poisson distribution. The Poisson distribution has the interesting feature that the mean is also the variance, and so as the mean increases, so does the variance. As such, the Poisson distribution is governed by the parameter, $\lambda$ and it distributes probabilities over non-negative integers. The Poisson distribution is [@pitman, p. 121]: 

$$P_{\lambda}(k) = e^{-\lambda}\mu^k/k!\mbox{, for }k=0,1,2,...$$
We can simulate $n=20$ draws from the Poisson distribution with `rpois()`, where $\lambda$ is the mean. The $\lambda$ parameter governs the shape of the distribution (@fig-poissondistributiontakingshape).

```{r}
rpois(n = 20, lambda = 3)
```

```{r}
#| eval: true
#| include: true
#| message: false
#| warning: false
#| fig-cap: "The Poisson distribution is governed by the value of the mean, which is the same as its variance"
#| label: fig-poissondistributiontakingshape

set.seed(853)

poisson_takes_shape <-
  tibble(
    lambda = c(),
    draw = c()
  )

number_of_each <- 100

for (i in c(0, 1, 2, 4, 7, 10, 15, 50, 100)) {
  draws_i <-
    tibble(
      lambda = c(rep(i, number_of_each)),
      draw = c(rpois(n = number_of_each, lambda = i))
    )

  poisson_takes_shape <- rbind(poisson_takes_shape, draws_i)
  rm(draws_i)
}

poisson_takes_shape |>
  ggplot(aes(x = draw)) +
  geom_density() +
  facet_wrap(
    vars(lambda),
    scales = "free_y"
  ) +
  theme_minimal() +
  labs(
    x = "Integer",
    y = "Density"
  )
```

For instance, if we look at the number of A+ grades that are awarded in each university course in a given term then for each course we would have a count.

```{r}
set.seed(853)

size_of_the_classes <- 26

count_of_A_plus <-
  tibble(
    # Thanks to Chris DuBois: https://stackoverflow.com/a/1439843
    department = c(rep.int("1", 26), rep.int("2", 26), rep.int("3", 26)),
    course = c(
      paste0("DEP_1_", letters),
      paste0("DEP_2_", letters),
      paste0("DEP_3_", letters)
    ),
    number_of_A_plus = c(
      rpois(n = size_of_the_classes, lambda = 5),
      rpois(n = size_of_the_classes, lambda = 10),
      rpois(n = size_of_the_classes, lambda = 20)
    )
  )

count_of_A_plus
```

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| fig-cap: "Simulated number of A+ grades in various classes across three departments"
#| label: fig-simgradesdepartments

count_of_A_plus |> 
  ggplot(aes(x = number_of_A_plus, y = department)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Number of A+ grades",
    y = "Department"
  ) +
  theme_classic()
```

Our simulated dataset has the number of A+ grades awarded by courses, which are structured within departments (@fig-simgradesdepartments). We can use `glm()` from base R to quickly get a sense of the data (@tbl-modelsummarypoisson).

```{r}
stopifnot(
  class(count_of_A_plus$department) == "character",
  class(count_of_A_plus$course) == "character",
  class(count_of_A_plus$number_of_A_plus) == "integer",
  nrow(count_of_A_plus) == 78
)

grades_model_base <-
  glm(
    number_of_A_plus ~ department,
    data = count_of_A_plus,
    family = "poisson"
  )
```

The interpretation of the coefficient on "department2" is that it is the log of the expected difference between departments. So we expect $\exp(0.7893) \approx 2.2$ and $\exp(1.4804) \approx 4.4$ additional A+ grades in departments 2 and 3, compared with department 1.

We can use `tidymodels` to estimate Poisson regression models with `poissonreg` [@poissonreg] (@tbl-modelsummarypoisson).

```{r}
library(poissonreg)

set.seed(853)

count_of_A_plus_split <-
  initial_split(count_of_A_plus, prop = 0.80)
count_of_A_plus_train <- training(count_of_A_plus_split)
count_of_A_plus_test <- testing(count_of_A_plus_split)

stopifnot(
  class(count_of_A_plus_train$department) == "character",
  class(count_of_A_plus_train$course) == "character",
  class(count_of_A_plus_train$number_of_A_plus) == "integer",
  nrow(count_of_A_plus_train) == 62
)

a_plus_model_tidymodels <-
  poisson_reg(mode = "regression") |>
  set_engine("glm") |>
  fit(
    number_of_A_plus ~ department,
    data = count_of_A_plus_train
  )
```

And finally, we can build a Bayesian model and estimate it with `rstanarm`. We put a tight prior on the coefficients because of the propensity for the Poisson distribution to expand them substantially (@tbl-modelsummarypoisson).

$$
\begin{aligned}
y_i &\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) & = \beta_0 + \beta_1 x_1 + \beta_2 x_2\\
\beta_0 & \sim \mbox{Normal}(0, 0.5)\\
\beta_1 & \sim \mbox{Normal}(0, 0.5)\\
\beta_2 & \sim \mbox{Normal}(0, 0.5)
\end{aligned}
$$

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

stopifnot(
  class(count_of_A_plus$department) == "character",
  class(count_of_A_plus$course) == "character",
  class(count_of_A_plus$number_of_A_plus) == "integer",
  nrow(count_of_A_plus) == 78
)

count_of_A_plus_rstanarm <-
  stan_glm(
    number_of_A_plus ~ department,
    data = count_of_A_plus,
    family = poisson(link = "log"),
    prior = normal(location = 0, scale = 0.5, autoscale = TRUE),
    prior_intercept = normal(location = 0, scale = 0.5, autoscale = TRUE),
    seed = 853
  )

saveRDS(
  count_of_A_plus_rstanarm,
  file = "count_of_A_plus_rstanarm.rds"
)
```

```{r}
#| eval: false
#| include: false
#| message: false
#| warning: false

# INTERNAL
saveRDS(
  count_of_A_plus_rstanarm,
  file = "outputs/model/count_of_A_plus_rstanarm.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

count_of_A_plus_rstanarm <-
  readRDS(file = "outputs/model/count_of_A_plus_rstanarm.rds")
```


```{r}
#| label: tbl-modelsummarypoisson
#| tbl-cap: "Forecasting and explanatory models of marathon times based on five-kilometer run times"

modelsummary(
  list(
    "base R" = grades_model_base,
    "tidymodels" = a_plus_model_tidymodels,
    "rstanarm" = count_of_A_plus_rstanarm
  ),
  statistic = "conf.int"
)
```

We can use `marginaleffects` [@marginaleffects] to help with interpreting these results.

```{r}
library(marginaleffects)
predicted_outcomes <- predictions(count_of_A_plus_rstanarm)

plot_cap(count_of_A_plus_rstanarm, condition = "department")

mfx <-marginaleffects(count_of_A_plus_rstanarm)
summary(mfx)
```


### Es in *Jane Eyre*

Inspired by @edgeworth1885methods, who made counts of the dactyls in Virgil's Aeneid (the data are available with `HistData::Dactyl` from @HistData), we could say, create counts, of the number of times the letter "E" or "e", appears in the first 10 lines of each chapter in *Jane Eyre* by Charlotte BrontÃ« using `gutenbergr` [@gutenbergr].

```{r}
#| eval: false
#| echo: true

library(gutenbergr)

gutenberg_id_of_janeeyre <- 1260

jane_eyre <-
  gutenberg_download(
    gutenberg_id = gutenberg_id_of_janeeyre,
    mirror = "https://gutenberg.pglaf.org/")

jane_eyre

write_csv(jane_eyre, "jane_eyre.csv")
```

```{r}
#| eval: false
#| echo: false

# INTERNAL

write_csv(jane_eyre, "inputs/jane_eyre.csv")
```

```{r}
#| eval: false
#| echo: true

jane_eyre <- read_csv(
  "jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

```{r}
#| eval: true
#| echo: false

# INTERNAL

jane_eyre <- read_csv(
  "inputs/jane_eyre.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

jane_eyre
```

We will consider only lines that have content, that is remove those that are just there for spacing. Then we can create counts of the number of times "e" or "E" occurs in that line, for the first 10 lines of each chapter.

```{r}
jane_eyre_reduced <-
  jane_eyre |>
  filter(!is.na(text)) |> # Remove empty lines
  mutate(chapter = if_else(str_detect(text, "CHAPTER") == TRUE, 
                           text,
                           NA_character_)) |> # Find start of chapter
  fill(chapter, .direction = "down") |> # Add chapter number to each line
  group_by(chapter) |> 
  mutate(chapter_line = row_number()) |> # Add line number of each chapter
  filter(!is.na(chapter),
         chapter_line %in% c(2:11)) |> # Start at 2 to get rid of text "CHAPTER I" etc
  select(text, chapter) |> 
  mutate(
    chapter = str_remove(chapter, "CHAPTER "),
    chapter = str_remove(chapter, "â€”CONCLUSION"),
    chapter = as.integer(as.roman(chapter))
  ) # Change chapters to integers

jane_eyre_reduced <- 
  jane_eyre_reduced |> 
  mutate(count_e = str_count(text, "e|E"))

jane_eyre_reduced
```

We can verify that the mean and variance of the number of es is roughly similar by plotting all of the data (@fig-janeecounts). In particular, the mean, in pink, is 6.9, and the variance, in blue, is 6.2.

```{r}
#| echo: true
#| eval: true
#| fig-cap: "Number of 'E' or 'e' in the first ten lines of each chapter in Jane Eyre"
#| label: fig-janeecounts
#| message: false
#| warning: false

mean_e <- mean(jane_eyre_reduced$count_e)
variance_e <- var(jane_eyre_reduced$count_e)

jane_eyre_reduced |> 
  ggplot(aes(y = chapter, x = count_e)) +
  geom_point(alpha = 0.5) + 
  geom_vline(xintercept = mean_e, linetype = "dashed", color = "#C64191") +
  geom_vline(xintercept = variance_e, linetype = "dashed", color = "#0ABAB5") +
  theme_minimal() +
  labs(
    x = "Number of e's per line for first ten lines",
    y = "Chapter"
  )
```

We could built a model.


```{r}
#| eval: false
#| echo: true
#| message: false
#| warning: false

library(rstanarm)
  
jane_e_counts <-
  stan_glm(
    count_e ~ chapter,
    data = jane_eyre_reduced,
    family = poisson(link = "log"),
    seed = 853
  )

saveRDS(
  jane_e_counts,
  file = "jane_e_counts.rds"
)
```

```{r}
#| eval: false
#| echo: false

# INTERNAL

saveRDS(
  jane_e_counts,
  file = "outputs/model/jane_e_counts.rds"
)
```

```{r}
#| eval: true
#| include: false
#| message: false
#| warning: false

library(rstanarm)

jane_e_counts <-
  readRDS(file = "outputs/model/jane_e_counts.rds")
```


We can compare our different models (@tbl-modelsummaryjanee).

```{r}
#| label: tbl-modelsummaryjanee
#| tbl-cap: "Forecasting and explanatory models of whether it is day or night, based on the number of cars on the road"

modelsummary::modelsummary(
  list(
    "rstanarm" = jane_e_counts
  ),
  statistic = "conf.int"
)
```



```{r}
pp_check(jane_e_counts)
```


### Negative binomial regression

One of the major restrictions with Poisson regression is the assumption that the mean and the variance are the same. We can relax this assumption to allow over-dispersion and should then use a close variant, negative binomial regression. But why are we going through all this trouble? Common practice, is to change from counts into a binary and then use logistic regression. The issue with this is that we are losing information when we group responses together. For this reason it may be worth going to the extra effort to consider more bespoke options such as these.

For instance, if we were to consider the number of "k"s in the those same lines of Jane then we would have many zeros (@fig-janeecountsk). 

```{r}
jane_eyre_reduced <- 
  jane_eyre_reduced |> 
  mutate(count_k = str_count(text, "k|K"))
```

```{r}
#| echo: true
#| eval: true
#| fig-cap: "Number of 'K' or 'k' in the first ten lines of each chapter in Jane Eyre"
#| label: fig-janeecountsk
#| message: false
#| warning: false

mean_k <- mean(jane_eyre_reduced$count_k)
variance_k <- var(jane_eyre_reduced$count_k)

jane_eyre_reduced |> 
  ggplot(aes(y = chapter, x = count_k)) +
  geom_point(alpha = 0.5) + 
  geom_vline(xintercept = mean_k, linetype = "dashed", color = "#C64191") +
  geom_vline(xintercept = variance_k, linetype = "dashed", color = "#0ABAB5") +
  theme_minimal() +
  labs(
    x = "Number of k's per line for first ten lines",
    y = "Chapter"
  )
```

Similarly, consider, somewhat morbidly, that every year each individual either dies or does not. From the perspective of a state, we could gather data on the number of people who died each year, by their cause of death. The Canadian province of Alberta, makes the number of deaths, by cause, since 2001, for the top 30 causes each year, available [here](https://open.alberta.ca/opendata/leading-causes-of-death). We can look at the distribution of these deaths, and we have removed the labels as some are quite extensive (@fig-albertacod). (Note that because some causes are not always in the top 30 each year, not all causes have the same number of appearances.)

:::{.callout-note}
## Oh, you think we have good data on that!

Cause of death
:::

```{r}
#| eval: true
#| echo: true
#| fig-cap: "Annual number of deaths for the top 30 causes, since 2001, for Alberta, Canada"
#| label: fig-albertacod
#| message: false
#| warning: false

library(tidyverse)

alberta_cod <- 
  read_csv("inputs/alberta_COD.csv") |> 
  janitor::clean_names() |> 
  add_count(cause) |> 
  filter(n == 21) |> 
  mutate(cause = str_trunc(cause, 15))

alberta_cod |> 
  ggplot(aes(x = total_deaths, y= cause)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(
    x = "Number of deaths",
    y = "Cause"
  )
```

One thing that we notice is that the mean, 665, is somewhat different to the variance, 227,152 (@tbl-ohboyalberta).


```{r}
#| echo: false
#| eval: true
#| label: tbl-ohboyalberta
#| tbl-cap: "Summary statistics of the number of deaths, by cause, in Alberta"

alberta_cod |>
  summarise(
    min = min(total_deaths),
    mean = mean(total_deaths),
    std_dev = sd(total_deaths),
    variance = var(total_deaths),
    max = max(total_deaths)
  ) |>
  knitr::kable(
    col.names = c("Minimum",
                  "Mean",
                  "Std dev",
                  "Variance",
                  "Maximum"),
    digits = 0,
    format.args = list(big.mark = ",")
  )
```





```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false

library(rstanarm)
  
cause_of_death_alberta_poisson <-
  stan_glm(
    total_deaths ~ cause,
    data = alberta_cod,
    family = poisson(link = "log"),
    seed = 853
  )

cause_of_death_alberta_neg_binomial <-
  stan_glm(
    total_deaths ~ cause,
    data = alberta_cod,
    family = neg_binomial_2(link = "log"),
    seed = 853
  )
```


```{r}
#| echo: false
#| eval: false

# INTERNAL

saveRDS(
  cause_of_death_alberta_poisson,
  file = "outputs/model/cause_of_death_alberta_poisson.rds"
)

saveRDS(
  cause_of_death_alberta_neg_binomial,
  file = "outputs/model/cause_of_death_alberta_neg_binomial.rds"
)
```

```{r}
#| eval: true
#| echo: false

cause_of_death_alberta_poisson <-
  readRDS(file = "outputs/model/cause_of_death_alberta_poisson.rds")

cause_of_death_alberta_neg_binomial <-
  readRDS(file = "outputs/model/cause_of_death_alberta_neg_binomial.rds")
```

We can compare our different models (@tbl-modelsummarypoissonvsnegbinomial) (if there is an error with `modelsummary::modelsummary()` for the negative binomial model, then it can sometimes be resolved by installing `broom.mixed`).

```{r}
#| label: tbl-modelsummarypoissonvsnegbinomial
#| tbl-cap: "Modelling the most prevelent cause of deaths in Alberta, 2001-2020"

library(broom.mixed)

modelsummary::modelsummary(
  list(
    "Posson" = cause_of_death_alberta_poisson,
    "Negative binomial" = cause_of_death_alberta_neg_binomial
  ),
  statistic = NULL
)
```

We could use posterior predictive checks to more clearly show that the negative binomial approach is a better choice for this circumstance (@fig-ppcheckpoissonvsbinomial).

```{r}
#| echo: true
#| eval: true
#| message: false
#| warning: false
#| label: fig-ppcheckpoissonvsbinomial
#| layout-ncol: 2
#| fig-cap: "Comparing posterior prediction checks for Poisson and negative binomial models"
#| fig-subcap: ["Poisson model", "Negative binomial model"]

pp_check(cause_of_death_alberta_poisson)

pp_check(cause_of_death_alberta_neg_binomial)
```



<!-- ```{r} -->
<!-- poisson <- loo(cause_of_death_alberta_poisson, cores = 2) -->
<!-- neg_binomial <- loo(cause_of_death_alberta_neg_binomial, cores = 2) -->

<!-- loo_compare(poisson, neg_binomial) -->
<!-- ``` -->







<!-- ### Proportional hazards -->

<!-- Could do like how long someone has to wait for something? -->




## Deploying models

Having done the work to develop a dataset and explore it with a model that we are confident can be used, we may wish to enable this to be used more widely thank just our own computer. There are a variety of ways of doing this, including:

- using the cloud,
- creating R packages,
- making `shiny` applications, and
- using `plumber` to create an API.

The general idea here is that we need to know, and allow others to come to trust, the whole workflow. That is what our approach to this point brings. After this, then we may like to use our model more broadly. Say we been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we would like to use the model to do something. For instance, setting up a website that allows a model to be used to generate an insurance quote given several inputs.

In this chapter, we begin by moving our compute from our local computer to the cloud. We then describe the use of R packages and Shiny for sharing models. That works well, but in some settings other users may like to interact with our model in ways that we are not focused on. One way to allow this is to make our results available to other computers, and for that we will want to make an APIs. Hence, we introduce `plumber` [@plumber], which is a way of creating APIs.

### Amazon Web Services

Apocryphally the cloud is just another name for someone else's computer. And while that is true to various degrees, for our purposes that is enough. Learning to use someone else's computer can be great for a number of reasons including:

1) Scalability: It can be quite expensive to buy a new computer, especially if we only need it to run something every now and then, but by using the cloud, we can just rent for a few hours or days. This allows use to amortize this cost and work out what we actually need before committing to a purchase. It also allows us to easily increase or decrease the compute scale if we suddenly have a substantial increase in demand.
2) Portability: If we can shift our analysis workflow from a local computer to the cloud, then that suggests that there is we are likely doing good things in terms of reproducibility and portability. At the very least, code can run both locally and on the cloud, which is a big step in terms of reproducibility.
3) Set-and-forget: If we are doing something that will take a while, then it can be great to not have to worry about our own computer needing to run overnight. Additionally, on many cloud options, open-source statistical software, such as R and Python, is either already available, or relatively easy to set-up.

That said, there are downsides, including:

- Cost: While most cloud options are cheap, they are rarely free. To provide an idea of cost, using a well-featured AWS instance for a few days, may end up being a few dollars. It is also easy to accidentally forget about something, and generate unexpectedly large bills, especially initially.
- Public: It can be easy to make mistakes and accidentally make everything public.
- Time: It takes time to get set-up and comfortable on the cloud.

When we use the cloud, we are typically running code on a "virtual machine" (VM). This is an allocation that is part of a larger collection of computers that has been designed to act like a computer with specific features. For instance, we may specify that our virtual machine has, say, 8 GB RAM, 128 storage, and 4 CPUs. The VM would then act like a computer with those specifications. The cost to use cloud options increases based on the VM specifications.

In a sense, we started with a cloud option, through our initial recommendation, in @sec-fire-hose of using RStudio Cloud, before we moved to our local computer in @sec-r-essentials. That cloud option was specifically designed for beginners. We will now introduce a more general cloud option: Amazon Web Services (AWS). Often a particular business will use a particular cloud option, such as Google, AWS, or Azure, but developing familiarity with one will make the use of the others easier.

Amazon Web Services is a cloud service from Amazon. To get started we need to create an AWS Developer account [here](https://aws.amazon.com/developer/) (@fig-awsone).

::: {#fig-ipums layout-nrow=2}

![AWS Developer website](figures/aws_one.png){#fig-awsone}

![AWS Developer console](figures/aws_two.png){#fig-awstwo}

![Launching an AWSinstance](figures/aws_three.png){#fig-awsthree}

![Establishing a key-pair](figures/aws_five.png){#fig-awsfive}

Overview of getting Amazon AWS set-up
:::

After we have created an account, we need to select a region where the computer that we will access is located. After this, we want to "Launch a virtual machine" with EC2 (@fig-awstwo).

The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, a local computer may be a MacBook running Monterey. Louis Aslett provides AMIs that are already set-up with RStudio and much else [here](http://www.louisaslett.com/RStudio_AMI/). We can either search for the AMI of the region that we registered for, or click on the relevant link on Aslett's website. For instance, to use the AMI set-up for the Canadian central region we search for "ami-0bdd24fd36f07b638". The benefit of using these AMIs is that they are set-up specifically for RStudio, but the trade-off is that they are a little outdated, as they were compiled in August 2020.

In the next step we can choose how powerful the computer will be. The free tier is basic computer, but we can choose better ones when we need them. At this point we can pretty much just launch the instance (@fig-awsthree). If we start using AWS more seriously we could go back and select different options, especially around the security of the account. AWS relies on key pairs. And so we will need to create a PEM and save it locally (@fig-awsfive). We can then launch the instance.

After a few minutes, the instance will be running. We can use it by pasting the "public DNS" into a browser. The username is "rstudio" and the password is the instance ID.

We should have RStudio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.

We do not need to install, say, `tidyverse`, instead we can just call the library and keep going. This is because this AMI comes with many packages already installed. We can see the list of packages that are installed with `installed.packages()`. For instance, `rstan` is already installed, and we could set-up an instance with GPUs if we needed.

Perhaps as important as being able to start an AWS instance is being able to stop it (so that we do not get billed). The free tier is pretty great, but we do need to turn it off. To stop an instance, in the AWS instances page, select it, then "Actions -> Instance State -> Terminate".






<!-- ## R packages -->

<!-- To this point we have largely been using R Packages to do things for us. However, another way is to have them loaded -->

<!-- DoSS Toolkit -->


<!-- ## Shiny -->



### Plumber and model APIs

The general idea behind the `plumber` package [@plumber] is that we can train a model and make it available via an API that we can call when we want a forecast. It is pretty great.

Just to get something working, let us make a function that returns "Hello Toronto" regardless of the output. Open a new R file, add the following, and then save it as "plumber.R" (you may need to install the `plumber` package if you have not done that yet).

```{r}
#| eval: false

library(plumber)

#* @get /print_toronto
print_toronto <- function() {
  result <- "Hello Toronto"
  return(result)
}
```

After that is saved, in the top right of the editor you should get a button to "Run API". Click that, and your API should load. It will be a "Swagger" application, which provides a GUI around our API. Expand the GET method, and then click "Try it out" and "Execute". In the response body, you should get "Toronto". 

To more closely reflect the fact that this is an API designed for computers, you can copy/paste the "request HTML" into a browser and it should return "Hello Toronto".


#### Local model

Now, we are going to update the API so that it serves a model output, given some input. We are going to follow @buhrplumber fairly closely.

At this point, we should start a new R Project. To get started, let us simulate some data and then train a model on it. In this case we are interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap.

```{r} 
#| warning: false
#| message: false
#| eval: false

library(tidyverse)
set.seed(853)

number_of_observations <- 1000

baby_sleep <-
  tibble(
    afternoon_nap_length = rnorm(number_of_observations, 120, 5) |> abs(),
    noise = rnorm(number_of_observations, 0, 120),
    night_sleep_length = afternoon_nap_length * 4 + noise,
  )

baby_sleep |>
  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Baby's afternoon nap length (minutes)",
    y = "Baby's overnight sleep length (minutes)"
  ) +
  theme_classic()
```

Let us now use `tidymodels` to quickly make a model.

```{r} 
#| warning: false
#| message: false
#| eval: false

library(tidymodels)

set.seed(853)

baby_sleep_split <- initial_split(baby_sleep, prop = 0.80)
baby_sleep_train <- training(baby_sleep_split)
baby_sleep_test <- testing(baby_sleep_split)

model <-
  linear_reg() |>
  set_engine(engine = "lm") |>
  fit(
    night_sleep_length ~ afternoon_nap_length,
    data = baby_sleep_train
  )

write_rds(x = model, file = "baby_sleep.rds")
```

At this point, we have a model. One difference from what you might be used to is that we have saved the model as an ".rds" file. We are going to read that in.

Now that we have our model we want to put that into a file that we will use the API to access, again called "plumber.R". And we also want a file that sets up the API, called "server.R". So make an R script called "server.R" and add the following content:

```{r}
#| eval: false

library(plumber)

serve_model <- plumb("plumber.R")
serve_model$run(port = 8000)
```

Then in "plumber.R" add the following content:

```{r}
#| eval: false

library(plumber)
library(tidyverse)

model <- readRDS("baby_sleep.rds")

version_number <- "0.0.1"

variables <-
  list(
    afternoon_nap_length = "A value in minutes, likely between 0 and 240.",
    night_sleep_length = "A forecast, in minutes, likely between 0 and 1000."
  )

#* @param afternoon_nap_length
#* @get /survival
predict_sleep <- function(afternoon_nap_length = 0) {
  afternoon_nap_length <- as.integer(afternoon_nap_length)

  payload <- data.frame(afternoon_nap_length = afternoon_nap_length)

  prediction <- predict(model, payload)

  result <- list(
    input = list(payload),
    response = list("estimated_night_sleep" = prediction),
    status = 200,
    model_version = version_number
  )

  return(result)
}
```

Again, after we save the "plumber.R" file we should have an option to "Run API". Click that and you can try out the API locally in the same way as before.


#### Cloud model

To this point, we have got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use [DigitalOcean](https://www.digitalocean.com). It is a charged service, but when you create an account, it will come with $100 in credit, which will be enough to get started.

This set-up process will take some time, but we only need to do it once. Install two additional packages that will assist here are `plumberDeploy` [@plumberdeploy] and `analogsea` [@citeanalogsea] (which will need to be installed from GitHub `remotes::install_github("sckott/analogsea")`).

Now we need to connect the local computer with the DigitalOcean account. 

```{r}
#| eval: false

library(analogsea)

account()
```

Now we need to authenticate the connection, and this is done using a SSH public key.

```{r}
#| eval: false

key_create()
```

What you want is to have a ".pub" file on our computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When we have the key on our local computer, then we can check this using `ssh`.

```{r}
#| eval: false
library(ssh)

ssh_key_info()
```

Again, this will all take a while to validate. DigitalOcean calls every computer that we start a "droplet". So if we start three computers, then we will have started three droplets. We can check the droplets that are running.
 
```{r}
#| eval: false

droplets()
```

If everything is set-up properly, then this will print the information about all droplets that you have associated with the account (which at this point, is probably none). We first create a droplet.

```{r}
#| eval: false
library(plumberDeploy)

id <- do_provision(example = FALSE)
```

Then we get asked for the SSH passphrase and then it will just set-up a bunch of things. After this we are going to need to install a whole bunch of things onto our droplet.

```{r}
#| eval: false

install_r_package(
  droplet = id,
  c(
    "plumber",
    "remotes",
    "here"
  )
)

debian_apt_get_install(
  id,
  "libssl-dev",
  "libsodium-dev",
  "libcurl4-openssl-dev"
)

debian_apt_get_install(
  id,
  "libxml2-dev"
)

install_r_package(
  id,
  c(
    "config",
    "httr",
    "urltools",
    "plumber"
  )
)

install_r_package(id, c("xml2"))
install_r_package(id, c("tidyverse"))
install_r_package(id, c("tidymodels"))
```

And then when that is finally set-up (it will take 30 minutes or so) we can deploy our API.

```{r}
#| eval: false

do_deploy_api(
  droplet = id,
  path = "example",
  localPath = getwd(),
  port = 8000,
  docs = TRUE,
  overwrite = TRUE
)
```








## Exercises and tutorial


### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: *A person is interested in the heights of all the buildings in London. They walk around the city counting the number of floors for each building.* Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation, along with three independent variables that are associated the count of the number of levels.
3. *(Acquire)* Please describe one possible source of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched.
5. *(Communicate)* Please write two paragraphs about what you did.
1. Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?
2. What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?
3. What is statistical bias?
4. If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model?
5. According to @greenland2016statistical, p-values test (pick one)?
    a. All the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).
    b. Whether the hypothesis targeted for testing is true or not.
    c. A dichotomy whereby results can be declared "statistically significant".
6. According to @greenland2016statistical, a p-value may be small because (select all)?
    a. The targeted hypothesis is false.
    b. The study protocols were violated.
    c. It was selected for presentation based on its small size. 
7. According to @obermeyer2019dissecting, why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)?
    a. The algorithm uses health costs as a proxy for health needs.
    b. The algorithm was trained on Reddit data.
8. When should we use logistic regression (pick one)?
    a. Continuous dependent variable.
    b. Binary dependent variable.
    c. Count dependent variable.
9. We are interested in studying how voting intentions in the recent US presidential election vary by an individual's income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?
    a. Whether the respondent is a US citizen (yes/no)
    b. The respondent's personal income (high/low)
    c. Whether the respondent is going to vote for Trump (yes/no)
    d. Who the respondent voted for in 2016 (Trump/Clinton)
10. We are interested in studying how voting intentions in the recent US presidential election vary by an individual's income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?
    a. The race of the respondent (white/not white)
    b. The respondent's marital status (married/not)
    c. Whether the respondent is registered to vote (yes/no)
    d. Whether the respondent is going to vote for Biden (yes/no)
11. Please explain what a p-value is, using only the term itself (i.e. "p-value") and words that are amongst the 1,000 most common in the English language according to the [XKCD Simple Writer](https://xkcd.com/simplewriter/). (Please write one or two paragraphs.)
12. The mean of a Poisson distribution is equal to its?
    a. Median.
    b. Standard deviation.
    c. Variance.
13. What is power (in a statistical context)?
14. According to @citemcelreath [p. 162] "Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for..." (please select one answer)? 
    a. overcomplicating models.
    b.  asking bad questions.
    c. using bad data.
15. Is a model that fits the small or large world more important to you, and why?


### Tutorial {.unnumbered}

**Option 1:**

Allow that the true data generating process is the normal distribution with mean of one, and standard deviation of 1. We obtain a sample of 1,000 observations using some instrument. Simulate the following situation:

1) Unknown to us, the instrument has a mistake in it, which means that it has a maximum memory of 900 observations, and begins over-writing at that point, so the final 100 observations are actually a repeat of the first 100. 
2) We employ a research assistant to clean and prepare the dataset. During the process of doing this, unknown to us, they accidentally change half of the negative draws to be positive. 
3) They additionally, accidentally, change the decimal place on any value between 1 and 1.1, so that, for instance 1 becomes 0.1, and 1.1 would become 0.11.
4) You finally get the cleaned dataset and are interested in understanding whether the mean of the true data generating process is greater than 0.

Discuss your analysis of the data, and the effect the issues have had, and finally some steps you can put in place to ensure actual analysis has a chance to flag some of these issues.

**Option 2:**

<!-- You are interested in studying the characteristics of people's friendship groups and how those characteristics relate to individual-level outcomes, particularly economic measures.  -->

<!-- You have at your disposal individual-level data sourced from a social media website, which contains information about social interactions (comments on posts, tags, etc) on the website, as well as a wide variety of individual-level characteristics.  -->

<!-- 1. While the social media website is very popular, not everyone in the population you are interested in has an account, and not everyone that has an account is active on the website. Given you are interested in economic measures, what are some possible issues with using these data to make inferences about the broader population? -->
<!-- 2. The data do not contain information on individual-level income; however, for around 20 per cent of the sample you have information on location of residence (down to "census block", a to very small area) and you know the median income of each census block. As such, you decide to estimate individual level income as follows: -->
<!--     a) Regress median income of each census block on a series of individual level characteristics (such as age, education, marital status, gender, ...)  -->
<!--     b) Use these estimates to predict the income of individuals that do not have location information -->
<!-- Briefly discuss the advantages and disadvantages of this approach, particularly in how it could affect the study of income characteristics of friendship groups.  -->

